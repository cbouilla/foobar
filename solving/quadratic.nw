\documentclass{article}

\usepackage[a4paper,vmargin=1in]{geometry}
\usepackage{noweb}
\usepackage{amsmath,amsfonts,amssymb}
\usepackage{parskip}
\usepackage{xspace}
\usepackage{hyperref}





\begin{document}

\title{Solving Instances of the \textsf{3XOR} Problem: the Quadratic Algorithm}
\author{Charles Bouillaguet}

\maketitle

\section{Introduction}

This file describes an implementation of the quadratic algorithm. 

The main function described in this file ``solves'' a task. Once all tasks are
solved, the original instance has been solved. 

<<*>>=
#include <inttypes.h>
#include <stdbool.h>
#include <stdlib.h>
#include <stdio.h>
#include <err.h>
#include "common.h"

<<Type definitions>>
<<Auxiliary functions>>
<<The main function>>


@ \section{Data structures}

This section describes a data structure with the following specification. 
$\mathcal{D}$ holds a small set of [[uint64_t]]. There will be many 
\emph{trials} ``does $x \in \mathcal{D}$?''. In $99.99999\%$ of the case, the 
answer is NO. So, the datastructure is optimized to answer as quickly as 
possible in this case. False positives are allowed (i.e. giving the answer YES 
while it is in fact NO), but false negatives are forbidden.

A (more expensive) deterministic check is performed when the answer is YES, so 
the rate of false positives has to be kept low.

Several options are possible :
\begin{enumerate}
\item Linear Hashing (no false positives)
\item Cuckoo Hashing (no false positives, faster, more complicated)
\item Bloom filter (some false positives)
\item Cuckoo Filter (some false positives)
\end{enumerate}

Let's begin by the end, with the deterministic check performed when the answer 
is YES. We use linear hashing. We assume that 0 does not belong to the hash 
table (we actually check that). The hash table is fully described by a correctly
initialized array of 64-bits integers and its size.

<<Type definitions>>=
struct hash_table_t {
	uint64_t mask;
	uint64_t *H;    // of size mask + 1
};


@ Probing the hash table is simple... But entails branch mispredictions.

<<Auxiliary functions>>=
bool hashtable_lookup(const struct hash_table_t *s, const uint64_t x)
{
	uint64_t h = x & s->mask;
	uint64_t probe = s->H[h];
	while (probe) {
		if (probe == x)
			return true;
		h = (h + 1) & s->mask;
		probe = s->H[h];
	}
	return false;
}

<<Auxiliary functions>>=
void hashtable_free(struct hash_table_t *s)
{
	free(s->H);
	free(s);
}


@ Building the hash table is not much more complicated. The function takes a 
sequence $L$ of 64-bits integers and returns a hash table. The correct size of 
the hash table is computed at run-time, so that the fill ratio is close to 0.5. 
This ensures that probing stops after $\approx 2.5$ probes.

<<Auxiliary functions>>=
struct hash_table_t * hashtable_build(uint64_t * const L, uint64_t lo, uint64_t hi)
{
	struct hash_table_t *s = malloc(sizeof(*s));
	<<Set [[size]] to the size of the hash table>>
	<<Initialize empty hash table>>
	for (uint64_t i = lo; i < hi; i++) {
		if (L[i] == 0)
			errx(1, "cannot insert 0 in hash table");
		<<insert [[L[i]]] into the hash table>>
	}
	s->H = H;
	s->mask = mask;
	return s;
}

@ To enable the use of efficient arithmetic, we use a table size which is a 
power of two. We thus determine the smallest power of two larger that $2n$.

<<Set [[size]] to the size of the hash table>>=
int tmp = 2*(hi - lo) + 1;
int i = 1;
while (tmp) {
	tmp >>= 1;
	i++;
}
int size = 1 << i;


<<Initialize empty hash table>>=
uint64_t mask = ((uint64_t) size) - 1;
uint64_t *H = malloc(size * sizeof(uint64_t));
if (H == NULL)
	err(1, "cannot allocate linear hash table");
for (int i = 0; i < size; i++)
	H[i] = 0;

@ Insertion is similar to probing: we find the first possible empty space, and 
we put the value there.

<<insert [[L[i]]] into the hash table>>=
uint64_t h = L[i] & mask;
while (H[h] != 0)
	h = (h + 1) & mask;
H[h] = L[i];



@ For now... let's use this as our final data structure.


\section{Processing Tasks}

 ``Solving the task'' means finding all pairs $(x, y)$ of 64-bits integers such
that $x \in A^{[i]}, y \in B^{[j]}$ and $x \oplus y \in C^{[i \oplus j]}$. 

<<The main function>>=
struct task_result_t * quadratic_task(struct context_t *ctx, struct task_t *task)
{
	bool debug = true;
	struct task_result_t *result;
	<<Prepare [[result]]>>
	<<Compute slices boundaries>>
	<<Allocate memory and load slices from disk>>
	<<Split the task into blocks and chunks>>
	<<Process all the blocks>>
	<<Release memory>>
	return result;
}


@ The number of solutions of a task is not known in advance, but it should be 
fairly small. We use a dynamic array to store them. Resizing is unlikely.
	

<<Prepare [[result]]>>=
result = malloc(sizeof(*result));
if (result == NULL)
	err(1, "cannot allocate task result object");
result->size = 0;
result->capacity = 128;
result->solutions = malloc(result->capacity * sizeof(struct solution_t));

@ When a new solution is found, it is appended to [[result]].

<<Report $x, y$ as a solution>>=
if (result->size == result->capacity) {
	result->solutions = realloc(result->solutions, 2 * result->capacity);
	if (result->solutions == NULL)
		err(1, "failed to re-alloc solutions array");
	result->capacity *= 2;
}
result->solutions[result->size].x = x;
result->solutions[result->size].y = y;
result->size++;

@ \subsection{Preparations}

As a result of the preprocessing, we should have three files containing all 
the hashes, as well as three index files. We assume that the indexes have 
already been loaded to memory, and that they are in the [[ctx]] object. The 
``depth'' of the indexes (i.e. largest $k$ such that $\mathrm{index}[2^k]$ is 
defined) must be known. We therefore assume that [[ctx->index_depth]] gives us 
the depth of the indexes, and that [[ctx->indexes]] are the indexes themselves.

Before loading data from disk, we must determine what slices to load. We simply
have to load $A^{[i]}, B^{[j]}$ and $C^{[i \oplus j]}$ where $0 \leq i, j < 2^k$.
The depth of the index is normally larger than $k$, so that we have to ``jump''
positions in the index.

<<Compute slices boundaries>>=
int k = task->k;
int depth = ctx->depth;
int task_step = 1 << (depth - k);
uint32_t *indexes[3];
uint32_t task_lo[3], task_hi[3];
for (int kind = 0;  kind < 3; kind++) {
	indexes[kind] = ctx->indexes[kind];
	int offset = task->idx[kind] * task_step;
	task_lo[kind] = indexes[kind][offset];
	task_hi[kind] = indexes[kind][offset + task_step];
	if (debug)
		fprintf(stderr, "task[%d] = %d:%d\n", kind, task_lo[kind], task_hi[kind]);
}

@ We assume that the hash files are already opened, and are accessible in [[ctx]].

<<Allocate memory and load slices from disk>>=
uint64_t *slice[3];
FILE **files = ctx->files;
for (int kind = 0;  kind < 3; kind++) {	
	uint32_t size = task_hi[kind] - task_lo[kind];
	slice[kind] = malloc(size * sizeof(uint64_t));
	if (slice[kind] == NULL)
		err(1, "cannot allocate hashes");
	if (fseek(files[kind], task_lo[kind] * sizeof(uint64_t), SEEK_SET))
		err(1, "cannot seek");
	size_t check = fread(slice[kind], sizeof(uint64_t), size, files[kind]);
	if (check != size)
		errx(1, "incomplete read");
}

<<Release memory>>=
for (int kind = 0;  kind < 3; kind++) {
	free(slice[kind]);
}

@ At this stage, the hashes needed to perform the task are loaded. Now, one 
problem is that the indexes refer to the full lists, and not to the slices we
have loaded. Because we have to split the task into blocks anyway, we will
compute \emph{block indexes} that refer to the task slices.

To split the task into a grid, we compute sub-slices boundaries. On $A$, this 
delimits blocks. On $B$ and $C$, this delimits chunks.

<<Split the task into blocks and chunks>>=
int l = ctx->l;
int grid_size = 1 << l;
uint32_t block_lo[3][grid_size]; 
uint32_t block_hi[3][grid_size]; 
int block_step = 1 << (depth - k - l);
for (int kind = 0; kind < 3; kind ++) {
	int offset = task->idx[kind] * task_step;
	uint32_t base = task_lo[kind];
	for (int it = 0; it < grid_size; it++) {
		block_lo[kind][it] = indexes[kind][offset + it * block_step] - base;
		block_hi[kind][it] = indexes[kind][offset + (it + 1) * block_step] - base;
	}
}

@ \subsection{Processing Blocks and Chunks}

In a CPU core, blocks are processed sequentially (on a GPU, many blocks would be 
processed simultaneously on all available multiprocessors). In a block, a small
slice of $A$ is loaded into a datastructure $\mathcal{D}$ with fast membership 
test. Then, all chunks exploit this datastructure.

<<Process all the blocks>>=
for (int u = 0; u < grid_size; u++) {
	<<Process block $u$>>
}

<<Process block $u$>>=
struct hash_table_t *D;
const uint32_t A_lo = block_lo[0][u];
const uint32_t A_hi = block_hi[0][u];
D = hashtable_build(slice[0], A_lo, A_hi);
for (int v = 0; v < grid_size; v++) {
	<<Process chunk $(u, v)$>>
}
hashtable_free(D);


@ Let's get down to the nitty-gritty of processing a chunk. It comes down to
probing the datastructure holding a small slice of $A$ will all pairs in the
matching slices of $B$ and $C$.

<<Process chunk $(u, v)$>>=
const uint64_t B_lo = block_lo[1][v];
const uint64_t B_hi = block_hi[1][v];
const uint64_t C_lo = block_lo[2][u ^ v];
const uint64_t C_hi = block_hi[2][u ^ v];

for (uint64_t r = B_lo; r < B_hi; r++) {
	const uint64_t x = slice[1][r];
	for (uint64_t s = C_lo; s < C_hi; s++) {
		const uint64_t y = slice[2][s];
		<<Check pair $(x, y)$>>
	}
}

<<Check pair $(x, y)$>>=
if (hashtable_lookup(D, x ^ y)) {
	<<Report $x, y$ as a solution>>
}

@ 
\end{document}