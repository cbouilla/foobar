\documentclass{book}
\usepackage{noweb}

\usepackage{amsmath,amsfonts,amssymb}
\usepackage{textcomp}
\usepackage{parskip}
\usepackage{geometry}
\usepackage{xspace}
\usepackage{hyperref}

\begin{document}

\setcounter{chapter}{4}
\chapter{Merger}

This program takes sub-dictionnary files and writes down a
\textbf{hash file} (i.e. a sequence of [[u64]]) in ascending order,
without duplicates. Optionnaly, the output can be randomly permuted.

<<*>>=
<<Header files to include>>
<<Auxiliary functions>>
<<The main program>>

@ We need the usual standard headers.

<<Header files to include>>=
#define _XOPEN_SOURCE 500
#include <stdio.h>
#include <stdlib.h>
#include <inttypes.h>
#include <string.h>
#include <err.h>
#include <getopt.h>
#include <sys/types.h>
#include <sys/stat.h>
#include <unistd.h>
#include <sys/time.h>
#include <assert.h>

#include "preprocessing.h"

@ We use a small function to measure the passage of time accurately.

<<Auxiliary functions>>=
double wtime()
{
	struct timeval ts;
	gettimeofday(&ts, NULL);
	return (double)ts.tv_sec + ts.tv_usec / 1E6;
}

<<Auxiliary functions>>=
void * load_dict(const char *filename, u64 *size_)
{
        struct stat infos;
        if (stat(filename, &infos))
                err(1, "fstat failed on %s", filename);
        u64 size = infos.st_size;
        assert(size % sizeof(struct dict_t) == 0);
        u64 *content = malloc(size);
        if (content == NULL)
                err(1, "failed to allocate memory");
        FILE *f = fopen(filename, "r");
        if (f == NULL)
                err(1, "fopen failed (%s)", filename);
        u64 check = fread(content, 1, size, f);
        if (check != size)
                errx(1, "incomplete read %s", filename);
        fclose(f);
        *size_ = size / sizeof(struct dict_t);
        return content;
}


<<The main program>>=
int main(int argc, char **argv)
{
	<<Process the command line>>
	<<Load all input files>>
	<<Sort input>>
	<<Deduplicate to auxiliary array>>
	if (randomize) {
		<<Randomly permute output>>
	}
	<<Dump output>>
	exit(EXIT_SUCCESS);
}

@ The name of input and output files are given on the command-line. Let us deal with the technicalities first.

<<Process the command line>>=
struct option longopts[3] = {
	{"output", required_argument, NULL, 'o'},
	{"randomize", no_argument, NULL, 'r'},
	{NULL, 0, NULL, 0}
};
char *out_filename = NULL;
bool randomize = false;
signed char ch;
while ((ch = getopt_long(argc, argv, "", longopts, NULL)) != -1) {
	switch (ch) {
	case 'o':
		out_filename = optarg;
		break;
	case 'r':
		randomize = true;
		break;
	default:
		errx(1, "Unknown option\n");
	}
}
if (out_filename == NULL)
	errx(1, "missing --output FILE");
if (optind >= argc)
	errx(1, "missing input filenames");
u32 P = argc - optind;
char **in_filenames = argv + optind;


<<Load all input files>>=
struct dict_t *dict[P];
u64 size[P];
u64 total_size = 0;
for (u32 i = 0; i < P; i++) {
	printf("* Loading %s\n", in_filenames[i]);
	dict[i] = load_dict(in_filenames[i], &size[i]);
	total_size += size[i];
}
struct dict_t *IN = malloc(total_size * sizeof(*IN));
total_size = 0;
for (u32 i = 0; i < P; i++) {
	memcpy(IN + total_size, dict[i], size[i] * sizeof(*IN));
	total_size += size[i];
	free(dict[i]);
}

@ We use a cheap strategy: sort, then deduplicate.

<<Auxiliary functions>>=
int cmp(const void *a_, const void *b_)
{
	struct dict_t *a = (struct dict_t *) a_;
	struct dict_t *b = (struct dict_t *) b_;
	return (a->hash > b->hash) - (a->hash < b->hash);
}


<<Sort input>>=
printf("--- Sorting\n");
qsort(IN, total_size, sizeof(*IN), cmp);

@ Duplicates are detected on output. We simply keep the previous value
actually written and discard an eventual output if it is equal to this value.


<<Deduplicate to auxiliary array>>=
u64 *OUT = malloc(total_size * sizeof(*OUT));
u64 n = 0;
int duplicates = 0;
int collisions = 0;
struct dict_t prev;
prev.hash = 0;
prev.preimage.nonce = 0;
prev.preimage.counter = 0;
for (u64 i = 0; i < total_size; i++) {
	if (IN[i].hash != prev.hash) {
		OUT[n++] = IN[i].hash;
	} else {
		/* diagnose duplicate */
		if ((IN[i].preimage.counter == prev.preimage.counter) 
					&& (IN[i].preimage.nonce == prev.preimage.nonce))
			duplicates++;
		else
			collisions++;
	}
	prev = IN[i];
}
printf("-- item processed = %" PRId64 ", hash collisions=%d, duplicate=%d\n", n, collisions, duplicates);


<<Randomly permute output>>=
for (u32 i = 0; i < n - 1; i++) {
        u32 j = i + (OUT[i] % (n - i));
        u64 x = OUT[i];
	OUT[i] = OUT[j];
        OUT[j] = x;
}
        

<<Dump output>>=
printf("-- Saving\n");
FILE *f_out = fopen(out_filename, "w");
if (f_out == NULL)
	err(1, "cannot open %s for writing", out_filename);
size_t check = fwrite(OUT, n, sizeof(*OUT), f_out);
if (check != n)
	err(1, "incomplete fwrite");
if (fclose(f_out))
	err(1, "fclose on %s", out_filename);


@ \end{document}
