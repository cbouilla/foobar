\documentclass{book}
\usepackage{noweb}

\usepackage{amsmath,amsfonts,amssymb}
\usepackage{textcomp}
\usepackage{parskip}
\usepackage{geometry}
\usepackage{tikz}
\usepackage{xspace}
\usepackage{hyperref}

\newcommand{\MPI}{\textsf{MPI}\xspace}

\begin{document}
\setcounter{chapter}{2}
\chapter{Splitter}

\section{Global Structure}

Splitting preimage files requires computing all the hashes, and this is CPU-bound. 
Therefore, we parallelize the hash computations.

The main strategy is as follows. A ``reader'' reads the input file and sends
blocks of (counter, nonce) pairs to ``mappers''. Each mapper assembles the
plaintext block, hash it, and dispatch it in one of its output buffers. When a
buffer is full, it is flushed to the ``writer''. Each of these workers is an independent process. 
Processes interact through the \MPI messaging library.

This program reads a preimage file, and compute all the hashes. It checks that
the (counter, nonce) pairs indeed yield hashes with 33 leading zero bits, and
dispatches the dictionnary items into sub-dictionnary files.

<<*>>=
<<Header files to include>>
<<Global variables>>
<<The main program>>

@ We need the usual standard headers.

<<Header files to include>>=
#define _XOPEN_SOURCE 500
#define _GNU_SOURCE
#include <stdio.h>
#include <stdlib.h>
#include <string.h>
#include <err.h>
#include <getopt.h>
#include <mpi.h>
#include "preprocessing.h"
#include "hasher.h"


<<The main program>>=
int main(int argc, char **argv)
{
	<<Initialization>>
	<<Process the command line>>
	<<Finish setup>>
	<<Start reader, mappers and writers>>
	MPI_Finalize();
	exit(EXIT_SUCCESS);
}

@ The possible command-line arguments are the size $k$ of the partitionning key, 
the output directory ([[dict/]]) and the preimage file. 
We enforce that these arguments are actually present.
The ``kind'' of the preimage file is inferred from its name.

<<Global variables>>=
struct option longopts[3] = {
	{"partitioning-bits", required_argument, NULL, 'b'},
	{"output-dir", required_argument, NULL, 'd'},
	{NULL, 0, NULL, 0}
};
int bits = -1;
char *output_dir = NULL;

<<Process the command line>>=
signed char ch;
while ((ch = getopt_long(argc, argv, "", longopts, NULL)) != -1) {
	switch (ch) {
	case 'b':
		bits = atoi(optarg);
		break;
	case 'd':
		output_dir = optarg;
		break;
	default:
		errx(1, "Unknown option\n");
	}
}
if (bits == -1) 
	errx(1, "missing required option --partitioning-bits");
if (optind != argc - 1)
	errx(1, "missing (or extra) filenames");
if (output_dir == NULL)
	errx(1, "missing required option --output-dir");
char *in_filename = argv[optind];
enum kind_t kind = file_get_kind(in_filename);

@ Before starting to dwelve into the code of the threads, we must set up a bit
of global context. The number of output files is $2^{[[bits]]}$. 
The number of mappers is the total number of processes minus $2$.

<<Global variables>>=
i32 rank, size;

<<Initialization>>=
MPI_Init(&argc, &argv);
MPI_Comm_size(MPI_COMM_WORLD, &size);
MPI_Comm_rank(MPI_COMM_WORLD, &rank);
<<Setup MPI datatype for preimages>>
<<Setup MPI datatype for dictionnary entries>>


@ To easily send / receive arrays of this though \MPI, we setup a
custom \MPI datatype. Please refer to the MPI guide for details.

<<Setup MPI datatype for preimages>>=
struct preimage_t sample[2];
MPI_Datatype PreimageStruct, PreimageType;
MPI_Datatype type[2] = {MPI_UINT64_T, MPI_UINT32_T};
int blocklen[2] = {1, 1};
MPI_Aint disp[2];
MPI_Aint base, sizeofentry;

/* compute displacements of structure components */
MPI_Get_address(&sample[0].counter, &disp[0]);
MPI_Get_address(&sample[0].nonce, &disp[1]);
MPI_Get_address(sample, &base);
disp[0] -= base;
disp[1] -= base;
MPI_Type_create_struct(2, blocklen, disp, type, &PreimageStruct);
MPI_Type_commit(&PreimageStruct);

/* If compiler does padding in mysterious ways, the following may be safer */
MPI_Get_address(sample + 1, &sizeofentry);
sizeofentry -= base;
MPI_Type_create_resized(PreimageStruct, 0, sizeofentry, &PreimageType);

/* quick safety check */
int x;
MPI_Type_size(PreimageType, &x);
if ((x != sizeof(struct preimage_t)) || (x != 12))
	errx(1, "data types size mismatch");


<<Setup MPI datatype for dictionnary entries>>=
struct dict_t sample2[2];
MPI_Datatype type2[2] = {MPI_UINT64_T, PreimageType};
MPI_Datatype DictStruct, DictType;

/* compute displacements of structure components */
MPI_Get_address(&sample2[0].hash, &disp[0]);
MPI_Get_address(&sample2[0].preimage, &disp[1]);
MPI_Get_address(sample2, &base);
disp[0] -= base;
disp[1] -= base;
MPI_Type_create_struct(2, blocklen, disp, type2, &DictStruct);
MPI_Type_commit(&DictStruct);

MPI_Get_address(sample2 + 1, &sizeofentry);
sizeofentry -= base;
MPI_Type_create_resized(DictStruct, 0, sizeofentry, &DictType);


@ \MPI processes are numbered starting at zero. The reader is process
0. The writer is process 1. The remaining processes are mappers. 
To easily distinguish between messages, we use tags.

<<Global variables>>=
static const int READER_REQUEST_TAG = 0;
static const int NONCE_BLOCK_TAG = 1;
static const int HASH_BLOCK_TAG = 2;
static const int EOF_TAG = 3;
static const int KEY_TAG = 4;
u32 n_mapper, n_slots;

<<Finish setup>>=
n_mapper = size - 2;
n_slots = 1 << bits;
if (rank == 0 && n_mapper <= 0)
	errx(1, "not enough MPI processes. Need 3, have %d", size);

@ Nonces and hashes are processed in batches. The output buffer size is 
much smaller, because with $k=10$, each mapper will have 1024 of these.

<<Global variables>>=
static const u32 READER_BUFFER_SIZE = 65536;
static const u32 WRITER_BUFFER_SIZE = 1024;

@ Thanks to the \MPI programming model, starting everything is easy.

<<Start reader, mappers and writers>>=
if (rank == 0) {
	<<Reader>>
} else if (rank == 1) {
	<<Writer>>
} else {
	<<Mapper>>
}


@

\section{Reading the Preimage Files}

The mapper threads will request preimage blocks from the reader. The reader
will reply with either a preimage block or an [[EOF]] message.

<<Reader>>=
printf("Reader started. %d mappers.\n", n_mapper);
u32 preimages_read = 0;
double start = MPI_Wtime();
double wait = 0;
FILE *f = fopen(in_filename, "r");
if (f == NULL)
	err(1, "fopen on %s", in_filename);
while (1) {
	<<Read a preimage block from [[f]] in [[buffer]]>>
	<<Wait for a request and send back [[buffer]]>>
	<<Print status report>>
}
fclose(f);
for (u32 i = 0; i < n_mapper; i++) {
	<<Wait for a request and send back [[EOF]]>>
}
printf("\nReader finished. %d preimages read, total wait = %.1f s\n", preimages_read, wait);


@ Reading the file is straightforward. We use a buffer of [[struct preimage_t]].

<<Read a preimage block from [[f]] in [[buffer]]>>=
struct preimage_t buffer[READER_BUFFER_SIZE];
size_t n_items = fread(buffer, sizeof(struct preimage_t), READER_BUFFER_SIZE, f);
if (ferror(f))
	err(1, "fread in reader");
if (n_items == 0 && feof(f))
	break;

@ Sending the block to the mapper is also quite simple. Because we accept
requests from anyone, we must be able to tell who asked us for a block. We
also use a specific "tag" for block requests. We use [[MPI_Bsend]], because it
allows us to get back to reading the file faster (at the expense of using a
bit more memory).


<<Wait for a request and send back [[buffer]]>>=
MPI_Status status;
double wait_start = MPI_Wtime();
MPI_Recv(NULL, 0, MPI_INT, MPI_ANY_SOURCE, READER_REQUEST_TAG, MPI_COMM_WORLD, &status);
wait += MPI_Wtime() - wait_start;
MPI_Send(buffer, n_items, PreimageType, status.MPI_SOURCE, NONCE_BLOCK_TAG, MPI_COMM_WORLD);
preimages_read += n_items;

@ Once all the files have been processed, the mappers must be told to stop
sending requests. We use an empty block with the EOF tag.

<<Wait for a request and send back [[EOF]]>>=
MPI_Status status;
MPI_Recv(NULL, 0, MPI_INT, MPI_ANY_SOURCE, READER_REQUEST_TAG, MPI_COMM_WORLD, &status);
MPI_Send(NULL, 0, MPI_INT, status.MPI_SOURCE, EOF_TAG, MPI_COMM_WORLD);


@ We implement a simple form of verbosity.

<<Print status report>>=
double megabytes = preimages_read * 1.1444091796875e-05;
double rate = megabytes / (MPI_Wtime() - start);
printf("\rPreimages read: %d (%.1f Mb, %.1f Mb/s)", preimages_read, megabytes, rate);
fflush(stdout);



@ \section{Hashing and Dispatching the Preimages}

Now come the mapper threads. They split input blocks into $2^[[bits]]$ output
buffers. When an output buffer is full, it is flushed to the writer.

<<Mapper>>=
int id = rank - 2;
<<Initialize output buffers>>
u32 n_processed = 0, n_invalid = 0;
while (1) {
	<<Request [[preimages]] from reader; if [[EOF]], then [[break]]>>
	for (int i = 0; i < n_preimages; i++) {
		<<Compute the hash; if invalid then [[continue]]>>
		<<Push the dictionnary entry to the output buffer [[slot]]>>
		if (output_size[slot] == WRITER_BUFFER_SIZE) {
			<<flush [[output[slot]]] to the writer>>
		}
	}
}
<<Flush all buffers and send [[EOF]] message to the writer>>
printf("Mapper %d finished. %d dictionnary entries transmitted. %d invalid.\n", id, n_processed, n_invalid);

@ Dealing with the buffer is as simple as in the reader. So is communication with the reader.

<<Initialize output buffers>>=
struct dict_t * output[n_slots];
u32 output_size[n_slots];
for (u32 i = 0; i < n_slots; i++) {
	output_size[i] = 0;
	output[i] = malloc(WRITER_BUFFER_SIZE * sizeof(struct dict_t));
	if (output[i] == NULL)
		err(1, "cannot alloc mapper output buffer");
}


@ When receiving a block from the reader, we only know an upper bound on its size. We must then query its actual size from \MPI.

<<Request [[preimages]] from reader; if [[EOF]], then [[break]]>>=
struct preimage_t preimages[READER_BUFFER_SIZE];
MPI_Status status;
MPI_Send(NULL, 0, MPI_INT, 0, READER_REQUEST_TAG, MPI_COMM_WORLD);
MPI_Recv(preimages, READER_BUFFER_SIZE, PreimageType, 0, MPI_ANY_TAG, MPI_COMM_WORLD, &status);
if (status.MPI_TAG == EOF_TAG)
	break;
i32 n_preimages;
MPI_Get_count(&status, PreimageType, &n_preimages);


@ A priori, the nonces are valid at difficulty 33. This means that the top 33
bits of the hash must be zero. This is checked by [[compute_full_hash]].

<<Compute the hash; if invalid then [[continue]]>>=
u32 full_hash[8];
if (!compute_full_hash(kind, preimages + i, full_hash)) {
	n_invalid++;
	continue;
}

<<Push the dictionnary entry to the output buffer [[slot]]>>=
u64 x = extract_partial_hash(full_hash);
u32 slot = extract_partitioning_key(bits, full_hash);
output[slot][output_size[slot]].hash = x;
output[slot][output_size[slot]].preimage.counter = preimages[i].counter;
output[slot][output_size[slot]].preimage.nonce = preimages[i].nonce;
output_size[slot] += 1;

@ To push the data to the writer, we first send [[slot]] and then only the actual data.
		
<<flush [[output[slot]]] to the writer>>=
MPI_Send(&slot, 1, MPI_INT, 1, KEY_TAG, MPI_COMM_WORLD);
MPI_Send(output[slot], output_size[slot], DictType, 1, HASH_BLOCK_TAG, MPI_COMM_WORLD);
n_processed += output_size[slot];
output_size[slot] = 0;


@ When we receive the [[EOF]] mark from the reader, we must flush incomplete
output buffers to the writer.

<<Flush all buffers and send [[EOF]] message to the writer>>=
for (u32 slot = 0; slot < n_slots; slot++) {
	<<flush [[output[slot]]] to the writer>>
}
MPI_Send(NULL, 0, MPI_INT, 1, EOF_TAG, MPI_COMM_WORLD);

@






\section{Writing sub-Dictionnaries to Disk}

It remains to describe the writer. The writer sequentialize writes, 
and deals with all the files. We just open the $2^k$ files simultaneously. 
For somewhat largish value of $k$, this may hit the OS restrictions. 
On my linux laptop, the default limit is 1024. Using [[ulimit -n 2048]] 
fixes the problem.

<<Writer>>=
u32 n_eof = 0;
FILE * f[n_slots];
<<Open all output files>>
printf("Writer ready\n");
while (n_eof < n_mapper) {
	<<Receive [[block]] from a mapper; if [[EOF]] then [[continue]]>>
	<<Write [[block]] to the correct file>>
}
<<Close all files>>
printf("Writer done.\n");

@ We derive the output filename from the input filename, the (optional) output
directory and its [[slot]]. The output files are opened in write mode, so they are 
truncated to size zero.

<<Open all output files>>=
for (u32 i = 0; i < n_slots; i++) {
	char out_filename[255];
	char *input_base = basename(in_filename);
	sprintf(out_filename, "%s/%03x/%s.unsorted", output_dir, i, input_base);
	f[i] = fopen(out_filename, "w");
	if (f[i] == NULL)
		err(1, "[writer] Cannot open %s for writing", out_filename);
}

@ When receiving a block from a mapper, we must distinguish between regular
and EOF messages. We must also observe the size of the hash block.

<<Receive [[block]] from a mapper; if [[EOF]] then [[continue]]>>=
MPI_Status status;
u32 slot;
struct dict_t block[WRITER_BUFFER_SIZE];
MPI_Recv(&slot, 1, MPI_INT, MPI_ANY_SOURCE, MPI_ANY_TAG, MPI_COMM_WORLD, &status);
if (status.MPI_TAG == EOF_TAG) {
	n_eof++;
	continue;
}
MPI_Recv(block, WRITER_BUFFER_SIZE, DictType, status.MPI_SOURCE, 
				HASH_BLOCK_TAG, MPI_COMM_WORLD, &status);
i32 n_entries;
MPI_Get_count(&status, DictType, &n_entries);

<<Write [[block]] to the correct file>>=
size_t tmp = fwrite(block, sizeof(struct dict_t), n_entries, f[slot]);
if (tmp != (size_t) n_entries)
	err(1, "fwrite writer (file %03x): %zd vs %d", slot, tmp, n_entries);

<<Close all files>>=
for (u32 i = 0; i < n_slots; i++)
	if (fclose(f[i]))
		err(1, "fclose writer file %03x", i);


@

\end{document}
