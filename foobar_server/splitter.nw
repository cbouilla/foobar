\documentclass{article}
\usepackage{noweb}

\usepackage{amsmath,amsfonts,amssymb}
\usepackage{textcomp}
\usepackage{parskip}
\usepackage{geometry}
\usepackage{tikz}
\usepackage{xspace}
\usepackage{hyperref}

\def\noweb{{\tt noweb\/}}
\newcommand{\join}{\bowtie}

\newcommand{\ZMQ}{\textsf{$\varnothing$MQ}\xspace}
\newcommand{\NN}{\textsf{nanomsg}\xspace}
\newcommand{\MPI}{\textsf{MPI}\xspace}
\newcommand{\OMP}{\textsf{OpenMP}\xspace}

\begin{document}

\title{Tools for the \texttt{FOOBAR}-modified Miner: Splitter}
\author{Charles Bouillaguet}

\maketitle

The combination of the \texttt{FOOBAR}-modified Miner and the
\texttt{FOOBAR}-server yields \emph{preimage files}, containing (counter,
nonce) pairs. In this document, we call this a \textbf{preimage}. Each pair
requires 12 bytes, and we expect to have 3 lists of $2^{31.7}$ entries each,
for a total storage space of 117Gbyte.

The nonce files must be checked (i.e. the corresponding hashes must have 32
zero bits). They must then be hashed, and 64 bits of each hash stored to disk.
This yields three hash lists, of $25.4$Gbyte each. These lists have to be
sorted and checked for duplicates. Thus in total, we expect to need $\approx
150$ Gbyte of storage.

Each preimage is associated to a 256-bit \textbf{full hash}, of which 33 at
least must be zero. From this full hash, we extract a subset of 64 (uniformly
distributed) bits that we call the \textbf{hash}.

For various reasons, we ensure that the hashes are unique. Because of the
large number of preimages, we might encounter accidental collisions, so we
must actively enforce this. We actually maintain a dictionnary $[[hash]]
\rightarrow [[preimage]]$, where the hashes are unique, and where it is fairly
easy to add new preimages and to extract all the hashes in sorted order.

The miner generates $\approx 550$ preimages per second, which means roughly
550 Mbyte per day of fresh data. Ideally, we would like the total amount of
work required to integrate this much data to the dictionnary to be as low as
possible.

We use the following strategy :

\begin{enumerate}  
\item When a new preimage file is ready, split it into several \textbf{split dictionnaries}
by partitionning the hashes according to most significant $b$ bits. This yields 
$2^b$ buckets. Potential duplicates are now confined into a single bucket.

\item Sort the newly formed split dictionnaries by hash. They must fit in RAM in 
order to do this easily, but they should be small.

\item For each bucket, perform a multiway merge on all split dictionnaries. 
Detect duplicate hashes and write down the hashes in order into a \textbf{hash file}.
\end{enumerate}

To do all this, a collection of tools is necessary. The tool described in this
file does the first step. We use \textsf{OpenSSL}'s implementation of
\textsf{SHA256}.

Concretely, a preimage file contains a sequence of [[struct preimage_t]]. A
split dictionnary contains a sequence of [[dict_t]].

\goodbreak

<<Type definitions>>=
struct preimage_t {
	int64_t counter;
	uint32_t nonce;
} __attribute__((packed));

struct dict_t {
	uint64_t hash;
	struct preimage_t preimage;
} __attribute__((packed));


@ The main strategy is as follows. A "reader" reads the input file and sends
blocks of (counter, nonce) pairs to "mappers". Each mapper assembles the
plaintext block, hash it, and dispatch it in one of each buffers. When a
buffer is full, it is flushed to a "writter".

We considered several options. 

OPTION A: each "worker" is a thread. Threads interract through (concurrent)
\emph{queues}. The reader thread pushes blocks to a single-writter multiple-
readers queue. The mapper threads pull from this queue and push to several
multiple-writer single-reader queues (one for each writer thread). The queues
are partial and bounded. These queues could be implemented by hand (using
locks and semaphores, for instance), or could be built using a messaging
middleware such as \ZMQ or \NN. Threads could be managed using the [[pthread]]
standard library or using something like \OMP.

OPTION B: each "worker" is a process. Processes interact through the \MPI
messaging library, using asynchronous IO if possible.

We chose option B, because it is simpler.

\section{Global Structure}

This program reads a preimage file, and compute all the hashes. It checks that
the (counter, nonce) pairs indeed yield hashes with 32 leading zero bits, and
dispatches the dictionnary items into split dictionnary files.

<<*>>=
<<Header files to include>>
<<Type definitions>>
<<Global variables>>
<<The main program>>

@ We need the usual standard headers.

<<Header files to include>>=
#define _XOPEN_SOURCE 500
#include <stdio.h>
#include <stdlib.h>
#include <stdbool.h>
#include <string.h>
#include <err.h>
#include <inttypes.h>
#include <getopt.h>
#include <assert.h>

#include <mpi.h>
#include "sha256.h"

<<The main program>>=
int main(int argc, char **argv)
{
	<<Initialization>>
	<<Process the command line>>
	<<Finish setup>>
	<<Start reader, mappers and writers>>
	MPI_Finalize();
	exit(EXIT_SUCCESS);
}

@ The possible command-line arguments are the kind of the blocks  ([[FOO]],
[[BAR]], [[FOOBAR]]), the $\log_2$ of the number of output hash files and the
input nonce filenames. We enforce that the arguments are actually present.

<<Global variables>>=
struct option longopts[3] = {
	{"kind", required_argument, NULL, 'k'},
	{"bits", required_argument, NULL, 'b'},
	{NULL, 0, NULL, 0}
};
int kind = -1;
int bits = -1;

<<Process the command line>>=
signed char ch;
while ((ch = getopt_long(argc, argv, "", longopts, NULL)) != -1) {
	switch (ch) {
	case 'k':
		kind = atoi(optarg);
		break;
	case 'b':
		bits = atoi(optarg);
		break;
	default:
		errx(1, "Unknown option\n");
	}
}
if (kind == -1) 
	errx(1, "missing required option --kind");
if (bits == -1) 
	errx(1, "missing required option --bits");
if (optind != argc - 1)
	errx(1, "missing (or extra) filenames");
char *in_filename = argv[optind];


@ Before starting to dwelve into the code of the threads, we must set up a bit
of global context. The number of output files (and of writter threads) if
$2^{[[bits]]}$. The number of mappers is the total number of processes minus
$[[n_writer]] + 1$.

<<Global variables>>=
int rank, size;

<<Initialization>>=
MPI_Init(&argc, &argv);
MPI_Comm_size(MPI_COMM_WORLD, &size);
MPI_Comm_rank(MPI_COMM_WORLD, &rank);
<<Setup MPI datatype for preimages>>
<<Setup MPI datatype for dictionnary entries>>


@ To easily send / receive arrays of this though \MPI, we setup a
custom \MPI datatype. Please refer to the MPI guide for details.

<<Setup MPI datatype for preimages>>=
struct preimage_t sample[2];
MPI_Datatype PreimageStruct, PreimageType;
MPI_Datatype type[2] = {MPI_UINT64_T, MPI_UINT32_T};
int blocklen[2] = {1, 1};
MPI_Aint disp[2];
MPI_Aint base, sizeofentry;

/* compute displacements of structure components */
MPI_Get_address(&sample[0].counter, &disp[0]);
MPI_Get_address(&sample[0].nonce, &disp[1]);
MPI_Get_address(sample, &base);
disp[0] -= base;
disp[1] -= base;
MPI_Type_create_struct(2, blocklen, disp, type, &PreimageStruct);
MPI_Type_commit(&PreimageStruct);

/* If compiler does padding in mysterious ways, the following may be safer */
MPI_Get_address(sample + 1, &sizeofentry);
sizeofentry -= base;
MPI_Type_create_resized(PreimageStruct, 0, sizeofentry, &PreimageType);

/* quick safety check */
int x;
MPI_Type_size(PreimageType, &x);
assert(x == sizeof(struct preimage_t));
assert(x == 12);


<<Setup MPI datatype for dictionnary entries>>=
struct dict_t sample2[2];
MPI_Datatype DictStruct, DictType;

/* compute displacements of structure components */
MPI_Get_address(&sample2[0].hash, &disp[0]);
MPI_Get_address(&sample2[0].preimage, &disp[1]);
MPI_Get_address(sample2, &base);
disp[0] -= base;
disp[1] -= base;
MPI_Type_create_struct(2, blocklen, disp, type, &DictStruct);
MPI_Type_commit(&DictStruct);

MPI_Get_address(sample2 + 1, &sizeofentry);
sizeofentry -= base;
MPI_Type_create_resized(DictStruct, 0, sizeofentry, &DictType);


@ Using \MPI, processes are numbered starting at zero. The reader is process
0. The writers are processes $1, \dots, [[n_writer]]$. The remaining processes
are mappers. To easily distinguish between messages, we use tags.

<<Global variables>>=
static const int READER_REQUEST_TAG = 0;
static const int NONCE_BLOCK_TAG = 1;
static const int HASH_BLOCK_TAG = 2;
static const int EOF_TAG = 3;

int n_writer, n_mapper;

<<Finish setup>>=
n_writer = 1 << bits;
n_mapper = size - n_writer - 1;
if (rank == 0 && n_mapper <= 0)
	errx(1, "not enough MPI processes. Need %d, have %d", 2 + n_writer, size);

@ Nonces and hashes are processed in batches.

<<Global variables>>=
static const int READER_BUFFER_SIZE = 65536;
<<Global variables>>=
static const int WRITER_BUFFER_SIZE = 32768;

@

\section{Reading the Preimage Files}

The mapper threads will request preimage blocks from the reader. The reader
will reply with either a preimage block or an [[EOF]] message.

<<Reader>>=
printf("Reader started. %d mappers and %d writers.\n", n_mapper, n_writer);
int preimages_read = 0;
double start = MPI_Wtime();
double wait = 0;
FILE *f = fopen(in_filename, "r");
if (f == NULL)
	err(1, "fopen on %s", in_filename);
while (1) {
	<<Read a nonce block from [[f]] in [[buffer]]>>
	<<Wait for a request and send back [[buffer]]>>
	<<Print status report>>
}
fclose(f);
for (int i = 0; i < n_mapper; i++) {
	<<Wait for a request and send back [[EOF]]>>
}
printf("\nReader finished. %d preimages read, total wait = %.1f s\n", preimages_read, wait);


@ Reading the file is straightforward. We use a buffer of [[struct nonce_msg_t]].

<<Read a nonce block from [[f]] in [[buffer]]>>=
struct preimage_t buffer[READER_BUFFER_SIZE];
size_t n_items = fread(buffer, sizeof(struct preimage_t), READER_BUFFER_SIZE, f);
if (ferror(f))
	err(1, "fread in reader");
if (n_items == 0 && feof(f))
	break;

@ Sending the block to the mapper is also quite simple. Because we accept
requests from anyone, we must be able to tell who asked us for a block. We
also use a specific "tag" for block requests. We use [[MPI_Bsend]], because it
allows us to get back to reading the file faster (at the expense of using a
bit more memory).


<<Wait for a request and send back [[buffer]]>>=
MPI_Status status;
double wait_start = MPI_Wtime();
MPI_Recv(NULL, 0, MPI_INT, MPI_ANY_SOURCE, READER_REQUEST_TAG, MPI_COMM_WORLD, &status);
wait += MPI_Wtime() - wait_start;
MPI_Send(buffer, n_items, PreimageType, status.MPI_SOURCE, NONCE_BLOCK_TAG, MPI_COMM_WORLD);
preimages_read += n_items;

@ Once all the files have been processed, the mappers must be told to stop
sending requests. We use an empty block with the EOF tag.

<<Wait for a request and send back [[EOF]]>>=
MPI_Status status;
MPI_Recv(NULL, 0, MPI_INT, MPI_ANY_SOURCE, READER_REQUEST_TAG, MPI_COMM_WORLD, &status);
MPI_Send(NULL, 0, MPI_INT, status.MPI_SOURCE, EOF_TAG, MPI_COMM_WORLD);


@ We implement a simple form of verbosity.

<<Print status report>>=
double megabytes = preimages_read * 1.1444091796875e-05;
double rate = megabytes / (MPI_Wtime() - start);
printf("\rNonces read: %d (%.1f Mb, %.1f Mb/s)", preimages_read, megabytes, rate);
fflush(stdout);

@

\section{Hashing and Dispatching the Preimages}

Now come the mapper threads. They split input blocks into [[n_writer]] output
buffers. When an output buffer is full, it is flushed to the corresponding
writer.

<<Mapper>>=
int id = rank - n_writer - 1;
<<Initialize output buffers>>
int n_processed = 0, n_invalid = 0;
while (1) {
	<<Request [[preimages]] from reader; if [[EOF]], then [[break]]>>
	for (int i = 0; i < n_preimages; i++) {
		<<Assemble the plaintext>>
		<<Compute the hash; if invalid then [[continue]]>>
		<<Push the dictionnary entry to the output buffer [[slot]]>>
		if (output_size[slot] == WRITER_BUFFER_SIZE) {
			<<flush [[output[slot]]] to writer \#[[slot]]>>
		}
	}
}
<<Flush all buffers and send [[EOF]] message to all writers>>
printf("\nMapper %d finished. %d dictionnary entries transmitted. %d invalid.\n", id, n_processed, n_invalid);

@ Dealing with the buffer is as simple as in the reader. So is communication with the reader.

<<Initialize output buffers>>=
struct dict_t output[n_writer][WRITER_BUFFER_SIZE];
int output_size[n_writer];
for (int i = 0; i < n_writer; i++)
	output_size[i] = 0;


@ When receiving a block from the reader, we only know an upper bound on its size. We must then query its actual size from \MPI.

<<Request [[preimages]] from reader; if [[EOF]], then [[break]]>>=
struct preimage_t preimages[READER_BUFFER_SIZE];
MPI_Status status;
MPI_Send(NULL, 0, MPI_INT, 0, READER_REQUEST_TAG, MPI_COMM_WORLD);
MPI_Recv(preimages, READER_BUFFER_SIZE, PreimageType, 0, MPI_ANY_TAG, MPI_COMM_WORLD, &status);
if (status.MPI_TAG == EOF_TAG)
	break;
int n_preimages;
MPI_Get_count(&status, PreimageType, &n_preimages);

@ Assembling the block is essentially the same code that has been injected into the modified \textsf{cgminer}.

<<Global variables>>=
static const char *PREFIXES[3] = {
	             "FOO-0x0000000000000000                                                          ",
		     "BAR-0x0000000000000000                                                          ",
		     "FOOBAR-0x0000000000000000                                                       "
		};

static const char NIBBLE[16] = {48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 65, 66, 67, 68, 69, 70};
 

<<Assemble the plaintext>>=
int64_t counter = preimages[i].counter;
uint32_t nonce = preimages[i].nonce;

char buffer[80];
memcpy(buffer, PREFIXES[kind], 80);
	
int j = (kind == 2) ? 25 : 22;
while (counter > 0) {
	int nibble = counter & 0x000f;
	counter >>= 4;
	buffer[j] = NIBBLE[nibble];
	j--;
}

uint32_t *block = (uint32_t *) buffer;
block[19] = __builtin_bswap32(nonce);

@ A priori, the nonces are valid at difficulty 33. This means that the top 33
bits of the hash must be zero.

<<Compute the hash; if invalid then [[continue]]>>=
unsigned char md[32];
uint32_t hash[8];
SHA256((unsigned char *) block, 80, md);
SHA256((unsigned char *) md, 32, (unsigned char *) hash);
bool valid = (hash[7] == 0x00000000) && ((hash[6] & 0x80000000) == 0x0000000000);
if (!valid) {
	n_invalid += 1;
	continue;
}


@ We must extract 64 bits of the hash from [[hash[5]]] and [[hash[6]]],
excluding the most-significant bit of [[hash[6]]] which is always zero. We
replace it with the most-significant bit of [[hash[4]]].

<<Push the dictionnary entry to the output buffer [[slot]]>>=
uint64_t x = (((uint64_t) hash[5]) << 32) ^ hash[6] ^ (hash[4] & 0x80000000);
int slot = bits ? (x >> (64 - bits)) : 0;
output[slot][output_size[slot]].hash = x;
output[slot][output_size[slot]].preimage = preimages[i];
output_size[slot] += 1;

@ IO is easy thanks to \MPI. Recall that writer \#[[slot]] is actually \MPI
process $[[slot]] + 1$.
		
<<flush [[output[slot]]] to writer \#[[slot]]>>=
MPI_Send(output[slot], output_size[slot], DictType, slot + 1, HASH_BLOCK_TAG, MPI_COMM_WORLD);
n_processed += output_size[slot];
output_size[slot] = 0;


@ When we receive the [[EOF]] mark from the reader, we must flush incomplete
output buffers to the writters.

<<Flush all buffers and send [[EOF]] message to all writers>>=
for (int slot = 0; slot < n_writer; slot++) {
	<<flush [[output[slot]]] to writer \#[[slot]]>>
	MPI_Send(NULL, 0, MPI_INT, slot + 1, EOF_TAG, MPI_COMM_WORLD);
}

@






\section{Writing Split Dictionnaries to Disk}

It remains to describe the writers, which is almost the simplest. The writer's
[[id]] is its number amongst writer threads. It is its \MPI rank minus one.
Each writer writes a single split dictionnary.

<<Writer>>=
int id = rank - 1;
int n_eof = 0;
<<Open output file [[f]]>>
while (n_eof < n_mapper) {
	<<Receive [[block]] from a mapper; if [[EOF]] then [[continue]]>>
	<<Write [[block]] to [[f]]>>
}
<<Close [[f]]>>
printf("Writer %d done.\n", id);

@ We derive the output filename from the writer's [[id]]. The output file is
opened in write mode, so it is truncated to size zero.

<<Open output file [[f]]>>=
char out_filename[255];
sprintf(out_filename, "%s.dict.%d", in_filename, id);
FILE *f = fopen(out_filename, "w");
if (f == NULL)
	err(1, "fopen writer %d", id);

@ When receiving a block from a mapper, we must distinguish between regular
and EOF messages. We must also observe the size of the hash block.

<<Receive [[block]] from a mapper; if [[EOF]] then [[continue]]>>=
struct dict_t block[WRITER_BUFFER_SIZE];
MPI_Status status;
MPI_Recv(block, WRITER_BUFFER_SIZE, DictType, MPI_ANY_SOURCE, MPI_ANY_TAG, MPI_COMM_WORLD, &status);
if (status.MPI_TAG == EOF_TAG) {
	n_eof++;
	continue;
}
int n_entries;
MPI_Get_count(&status, DictType, &n_entries);

<<Write [[block]] to [[f]]>>=
size_t tmp = fwrite(block, sizeof(struct dict_t), n_entries, f);
if (tmp != (size_t) n_entries)
	err(1, "fwrite writer %d: %zd vs %d", id, tmp, n_entries);

<<Close [[f]]>>=
if (fclose(f))
	err(1, "fclose writer %d", id);

@

\section{Putting it all Toghether}

<<Start reader, mappers and writers>>=
if (rank == 0) {
	<<Reader>>
} else if (rank < 1 + n_writer) {
	<<Writer>>
} else {
	<<Mapper>>
}

@

\end{document}
