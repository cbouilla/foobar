\documentclass{book}

\usepackage{geometry}
\usepackage{noweb}
\usepackage{amsmath,amsfonts,amssymb}
\usepackage{parskip}
\usepackage{xspace}
\usepackage{hyperref}
\usepackage{tikz}
\usetikzlibrary{patterns}

\newcommand{\join}{\bowtie}
\newcommand{\OMP}{\textsf{OpenMP}\xspace}

\begin{document}

\setcounter{chapter}{2}
\chapter{The Iterated Joux Algorithm}

This file describes an implementation of the iterated Joux algorithm.  
The main function described in this file ``solves'' a (fine) task. 



<<*>>=
#include <math.h>
#include <stdlib.h>
#include <stdio.h>
#include <assert.h>
#include <err.h>
#include <sys/types.h>
#include <sys/stat.h>
#include <byteswap.h>
#include <strings.h>

#include <omp.h>
#include <papi.h>

#include "common.h"
#include "../quadratic/datastructures.h"
#include "../preprocessing/preprocessing.h"


<<Type definitions>>
<<Forward declarations>>
<<Auxiliary functions>>
<<The main function>>

@ \section{Processing Tasks}

The technical part consists in computing $AM \join_\ell BM$. All the code is organized
around this difficulty.

We use a \emph{partitioning} (hash) join. To compute $A \join B$, we divide 
$A$ and $B$ into partitions $(A_i)$ and $(B_j)$ such that
$A_i \join B_j = \emptyset$ when $i \neq j$. Radix partitioning puts in $A_i$
all entries from $A$ that begin with $i$ (the $\ell$ most-significant bits
of each key in $A_i$ is $i$). If we divide $A$ using $p$ key bits, we end up
with $2^p$ partitions. 

Our code is multi-threaded, in order to use the hardware threads of the BG/Q
and obtain a higher resource utilization ratio. An empirical study shows that
it is a bit faster to do the matrix multiplication first, and only then
partition the result. One likely cause is that both processes compete for the
L1 cache. The product must also be multi-threaded to reach 1 instruction/cycle.

The usual way to do radix-partitioning does two passes over the input: a first
pass counts the number of elements in each output bucket, then a second pass
actually dispatches the data.

We choose to do away with the first pass at the expense of a small increase in
storage. Each one of the $T$ threads directly dispatches its slice of the input
into $2^p$ \emph{thread-private} buckets. Each partition $A_i$ is then
composed of $T$ such thread-buckets. 

The drawback of this approach is that the size of each thread-bucket is not
known in advance. Because the keys are random, we may safely overestimate it
though. If $N$ denotes the size of the input list, then each output bucket
receives on average $\mu = N / (2^p T)$ entries. A Chernoff bound tells  us
that if $X$ denotes an actual thread-bucket size, 
\[ 
	\mathbb{P}\left[X > (1 + \delta)\mu \right] 
	< 
	\left( \frac{e^\delta}{(1+\delta)^{1+\delta}}\right)^\mu.
\]

We want to choose a safety margin $\delta$ such that the possibility of overflow
is remote (say $2^{-100}$). It is then enough to choose
$\delta \gets \sqrt{210 / \mu}$ (this is a loose bound). As long as $N$ is 
large enough, a small $\delta$ (20\% or less) is sufficient, and the space 
overhead is limited. 

We use $p=10$, which is the largest value for which partitioning is fast. The
value of $\ell$ must be somewhat greater than $p$, otherwise a problem occurs in
the (hash) join: the number of bits used to compute a hash value becomes smaller
than the number of hash adresses.

Similarly, the value of $\ell$ must be carefully chosen: increasing it makes the
join output smaller and reduces the number of probes in $CM$, but it increases
the number of slices to process.

When allocating the memory, we enforce alignement on a 64-byte boundary 
(the size of a cache line on most CPUs).

<<Auxiliary functions>>=
static const u32 CACHE_LINE_SIZE = 64;
u64 ROUND(u64 s)
{
	return CACHE_LINE_SIZE * ceil(((double) s) / CACHE_LINE_SIZE);
}

u64 chernoff_bound(u64 N, u32 n_buckets)
{
	double mu = ((double) N) / n_buckets;
	double delta = sqrt(210 / mu);
	return ROUND(N * (1 + delta) / n_buckets);
}


@ Given an input array [[IN]] of size [[N]], we will therefore allocate an array 
[[OUT]] of size $O \gets \left\lceil(1 + \delta)N\right\rceil$. This output 
array is split into $2^p$ ``prefix-zones'' of size $[[psize]] = [[output_size]] / 2^p$.
Each prefix-zone is itself split into $T$ ``thread-zone'' of size
$[[tsize]] = [[psize]] / T$. Each thread writes the entries it reads from the
input into his own thread-zones in all prefix-zones. 

\textbf{TODO : VERIFY.}

We ensure that the ``thread-zones'' have a size which is \textbf{NOT} a
multiple of [[CACHE_LINE_SIZE]]. This way, the associativity of the cache does
not cause problems. Another drawback is that when partitioning is over, the
data inside each  ``prefix zone'' are not contiguous.

@ \begin{tikzpicture}[>=latex]
  \draw[thick,dotted] (0, 0) -- (0, -1);
  \draw[thick,dotted] (3, 0) -- (3, -1);
  \draw[thick] (0, 0) -- (0, 8);
  \draw[thick] (3, 0) -- (3, 8);
  \draw[thick] (0, 8) -- (3, 8);
  \draw (0, 6) -- +(3, 0);
  \draw (0, 4) -- +(3, 0);
  \draw (0, 2) -- +(3, 0);
  \draw (0, 0) -- +(3, 0);
  \path (0, 8) rectangle node {Thread 0} +(3, -2);
  \path (0, 6) rectangle node {Thread 1} +(3, -2);
  \path (0, 4) rectangle node {Thread 2} +(3, -2);
  \path (0, 2) rectangle node {Thread 3} +(3, -2);

  \newlength{\espacehoriz}
  \setlength{\espacehoriz}{8cm}
  
  \begin{scope}[xshift=\espacehoriz,font=\footnotesize]
    \draw[thick,dotted] (0, 0) -- (0, -1);
    \draw[thick,dotted] (3, 0) -- (3, -1);
    \draw[thick] (0, 0) -- (0, 8);
    \draw[thick] (3, 0) -- (3, 8);
    \draw[thick] (0, 8) -- (3, 8);
    
    \draw[ultra thick] (0, 4) -- +(3, 0);
    \draw[ultra thick] (0, 0) -- +(3, 0);

    \foreach \i/\j in {8cm/0.65, 7cm/0.7, 6cm/0.6, 5cm/0.2, 4cm/0.9, 3cm/0.5, 2cm/0.2, 1cm/0.7}
    	\draw[pattern=north east lines] (0, \i-1cm) rectangle (3, \i-\j*1cm);

    \draw[<->] (4.5, 8) -- node[above, sloped] {[[psize]]} +(0, -4);
    \draw[<->] (4.5, 4) -- node[above, sloped] {[[psize]]} +(0, -4);
    \foreach \i in {0, ..., 7} {
    	\draw[<->] (3.75, \i + 1) -- node[above, sloped] {[[tsize]]} +(0, -1);
    	\draw[thick] (0, \i) -- +(3, 0);
    }
  \end{scope}

  \begin{scope}[shorten >=1mm]
    \draw[->] (3, 7) -- (\espacehoriz, 8 - 0.65 / 2);
    \draw[->] (3, 5) -- (\espacehoriz, 7 - 0.7 / 2);
    \draw[->] (3, 3) -- (\espacehoriz, 6 - 0.6 / 2);
    \draw[->] (3, 1) -- (\espacehoriz, 5 - 0.2 / 2);
    %
    \draw[->] (3, 7) -- (\espacehoriz, 4 - 0.9 / 2);
    \draw[->] (3, 5) -- (\espacehoriz, 3 - 0.5 / 2);
    \draw[->] (3, 3) -- (\espacehoriz, 2 - 0.2 / 2);
    \draw[->] (3, 1) -- (\espacehoriz, 1 - 0.7 / 2);
  \end{scope}
\end{tikzpicture}

The right value of $p$ must be found by trial-and-error. If $p$ is too high,
partitioning will be slow. If $p$ is too low, the partitions will be too large
to  fit in any cache and doing the sub-joins will be harder. Empirical studies
show that $p = 10$ is the largest good value.

We use a global array ([[count]]) to maintain de state of all the 
thread-buckets. Once partitioning is done, the items read by thread $t$ and
sent to the $i$-th partition are in [[scratch]] between index 
[[t * tsize + i * psize]] (inclusive) and [[count[t * fan_out + i]]] 
(exclusive). The rationale is that the [[count]]s used by each thread during
dispatching are contiguous in memory.

Once the partioning is done, we now have to join much smaller sub-slices of 
the input lists.

<<The main function>>=
struct task_result_t * iterated_joux_task_v3(const char *hash_dir, 
	                const char *slice_dir, struct jtask_id_t *task, u32 p)
{
	static const bool task_verbose = true;
	static const bool slice_verbose = false;
	struct task_result_t *result = result_init();
	double start = wtime();
	if (task_verbose)
		printf("Loading...\n");
	<<Load hash for $A, B$ from disk>>
	<<Load slices of $C$ from disk>>
	if (task_verbose)
		printf("Permuting...\n");
	<<Randomly permute $A$ and $B$>>
	<<Prepare context>>
	if (task_verbose) {
		<<Display task info>>
	}
	struct slice_t *slice = slices;
	u32 i = 0;
	while (1) {
		process_slice_v3(&self, slice, slice_verbose);
		i++;
		<<Advance to next slice; [[break]] when done>>
	}
	<<Release memory>>
	if (task_verbose) {
		<<Display task timing>>
	}
	return result;                           
}


@ Because of our preprocessing, the hash files are sorted. The input lists are
thus highly non-random. In order to be able to rely on the randomness of the
inputs, we re-randomize them by randomly permuting $A$ and $B$.

<<Randomly permute $A$ and $B$>>=
#pragma omp parallel for schedule(static)
for (u32 k = 0; k < 2; k++)
	for (u32 i = 0; i < n[k] - 1; i++) {
		u32 j = i + (L[k][i] % (n[k] - i));
		u64 x = L[k][i];
		L[k][i] = L[k][j];
		L[k][j] = x;
	}

                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             
<<Display task timing>>=
double task_duration = wtime() - start;
u32 n_coarse_tasks = 1 << (2 * task->k);
double total = task_duration * n_coarse_tasks * task->k2;
double Mvolume = self.volume * 9.5367431640625e-07;
printf("Task duration: %.1f s\n", task_duration);
printf("Total volume: %.1fMitem\n", Mvolume);
printf("Est. total time: %.3e h\n", total / 3600);
printf("Breakdown:\n");
printf("* GEMM:      \tT = %d, \ttime = %.2fs\trate = %.2fMitem/s\n", 
	self.T_gemm, 1e-6 * self.gemm_usec, self.volume / (self.gemm_usec / 1.048576));
printf("* partition: \tT = %d, \ttime = %.2fs\trate = %.2fMitem/s\n", 
	self.T_part, 1e-6 * self.part_usec, self.volume / (self.part_usec / 1.048576));
printf("* subjoin:   \tT = %d, \ttime = %.2fs\trate = %.2fMitem/s\n", 
	self.T_subj, 1e-6 * self.subj_usec, self.volume / (self.subj_usec / 1.048576));

/*
printf("* GEMM:      \ttime = %.2fs\tIPC = %.2f\tinstr/item = %.1f\tcycles/item = %.1f\n", 
	1e-6 * self.gemm_usec, (1.0 * self.gemm_instr) / self.gemm_cycles,
	(1.0 * self.gemm_instr) / self.volume, (1.0 * self.gemm_cycles) / self.volume);
printf("* partition: \ttime = %.2fs\tIPC = %.2f\tinstr/item = %.1f\tcycles/item = %.1f\n", 
	1e-6 * self.part_usec, (1.0 * self.part_instr) / self.part_cycles,
	(1.0 * self.part_instr) / self.volume, (1.0 * self.part_cycles) / self.volume);
printf("* subjoin:   \ttime = %.2fs\tIPC = %.2f\tinstr/item = %.1f\tcycles/item = %.1f\n", 
	1e-6 * self.subj_usec, (1.0 * self.subj_instr) / self.subj_cycles,
	(1.0 * self.subj_instr) / self.volume, (1.0 * self.subj_cycles) / self.volume);
*/

@ \section{Preparations}

Preparations include loading the data from disk. We expected a fair amount of 
data (say, 4Gb). Therefore this may take some time. In the future, we would
use this time to do something useful (try to precompute some matrices ?).

This utility function loads a sequence of [[u64]]. The file is assumed to be
in little-endian byte order. Therefore, the [[u64]] are byteswapped if the
machine is big-endian.

<<Forward declarations>>=
void * load_file(const char *filename, u64 *size_);

<<Auxiliary functions>>=
void * load_file(const char *filename, u64 *size_)
{
	struct stat infos;
	if (stat(filename, &infos))
		err(1, "fstat failed on %s", filename);
	u64 size = infos.st_size;
	assert ((size % 8) == 0);
	u64 *content = aligned_alloc(64, size);
	if (content == NULL)
		err(1, "failed to allocate memory");
	FILE *f = fopen(filename, "r");
	if (f == NULL)
		err(1, "fopen failed (%s)", filename);
	u64 check = fread(content, 1, size, f);
	if (check != size)
		errx(1, "incomplete read %s", filename);
	fclose(f);
	*size_ = size;
	<<if big-endian, byte-swap [[content]]>>
	return content;
}

<<if big-endian, byte-swap [[content]]>>=
if (big_endian()) {
	printf("Byte-swapping...\n");
	#pragma omp parallel for schedule(static)
	for (u32 i = 0; i < size / 8; i++)
		content[i] = bswap_64(content[i]);
}

@ Loading $A$ and $B$ is easy: we just load the whole hash file.

<<Load hash for $A, B$ from disk>>=
u64 *L[2];
u64 n[2];
for (u32 k = 0;  k < 2; k++) {
	<<Determine [[filename]]>>
	L[k] = load_file(filename, &n[k]);
	assert((n[k] % 8) == 0);
	n[k] /= 8;
}

@ This ought to be factored: it is not DRY wrt quadratic.

<<Determine [[filename]]>>=
char filename[255];
char *kind_name[3] = {"foo", "bar", "foobar"};
sprintf(filename, "%s/%s.%03x", hash_dir, kind_name[k], task->idx[k]);

@ Loading $C$ is a bit different, because the slice of $C$ we are concerned
with is contained in a slice file.

<<Load slices of $C$ from disk>>=
char filename[255];
sprintf(filename, "%s/%03x-%05x", slice_dir, task->idx[2], task->r);
struct slice_t * slices;
u64 slices_size;
slices = load_file(filename, &slices_size);


@ When allocating the memory, we enforce alignement on a 64-byte boundary  (the
size of a cache line on most CPUs). For this, we use
[[aligned_alloc]], available in \textsf{C11}, or an equivalent wrapper.


@ \subsection{Other Preparations (Scratch space, etc.)}

We must allocate all the scratch space needed for the multi-threaded
partitioning step. We could potentially take it on the stack, but this is 
going to be large (several Gb), so we figure that we may as well allocate it 
on the heap.

<<Type definitions>>=
struct side_t {
	u64 *L;               /* input list */
	u32 n;                /* size of L */
	
	/**** multi-threaded partitioning ****/
	u32 psize;            /* capacity of partitions */
	u32 tsize;            /* capacity of thread-private buckets */
	u64 *LM;              /* scratch space for the matrix product */
	u64 *scratch;         /* scratch space for partitioning */
	u32 *count;           /* counters for dispatching */
	u32 partition_size;   /* upper-bound on the actual number of items in
	                         a partition */
};

@ We define a [[context]] to pass all arguments to subfunctions.

<<Type definitions>>=
struct context_t {
	/**** input ****/
	u64 *L[2];
	u32 n[2];

	/**** nicely presented input ****/
	struct side_t side[2];

	/**** tuning parameters ****/
	u32 T_gemm, T_part, T_subj;         /* number of threads */
	u32 p;         /* bits used in partitioning */

	/* performance measurement */
	u64 volume;
	u64 gemm_usec, part_usec, subj_usec;
	long long gemm_instr, gemm_cycles;
	long long part_instr, part_cycles;
	long long subj_instr, subj_cycles;

	/**** output ****/
	struct task_result_t *result;
};

@ As discussed in the introduction, we need some scratch space to hold the 
partition data structure for both $A$ and $B$. The [[struct side_t]] object
is made for this.

<<Auxiliary functions>>=
void prepare_side(struct context_t *self, u32 k, bool verbose)
{
	u32 T = self->T_part;
	u32 n = self->n[k];
	u32 fan_out = 1 << self->p;
	// TODO : verifier si l'alignement de tsize sur 64 n'est pas une connerie.
	u32 tsize = chernoff_bound(n, T * fan_out);
	u32 psize = tsize * T;
	u32 scratch_size = psize * fan_out;
	u32 partition_size = chernoff_bound(n, fan_out);
	
	u64 *scratch = aligned_alloc(CACHE_LINE_SIZE, sizeof(u64) * scratch_size);
	if (scratch == NULL)
		err(1, "failed to allocate scratch space");
	u64 *LM = aligned_alloc(CACHE_LINE_SIZE, sizeof(u64) * n);
	if (LM == NULL)
		err(1, "failed to allocate LM");
	u32 count_size = ROUND(sizeof(u32) * T * fan_out);
	u32 *count = aligned_alloc(CACHE_LINE_SIZE, count_size);
	if (count == NULL)
		err(1, "failed to allocate count");
	
	struct side_t *side = &self->side[k];
	side->L = self->L[k];
	side->n = n;
	side->tsize = tsize;
	side->psize = psize;
	side->scratch = scratch;
	side->LM = LM;
	side->count = count;
	side->partition_size = partition_size;

	if (verbose) {
		printf("side %d, n=%d, T=%d\n", k, n, T);
		printf("========================\n");
		double expansion = (100.0 * (scratch_size - n)) / n;
		printf("|scratch| = %d items (expansion = %.1f %%), tisze=%d, psize=%d, part_size=%d\n", 
			scratch_size, expansion, tsize, psize, partition_size);
		double st_part = (9.765625e-04 * n * 8) / fan_out;
		printf("Expected partition size = %.1f Kb\n", st_part);
	}
}

<<Prepare context>>=
struct context_t self;
for (u32 k = 0; k < 2; k++) {
	self.n[k] = n[k];
	self.L[k] = L[k];
}
self.T_gemm = 4;
self.T_part = 2;
self.T_subj = 3; 
self.p = p;
self.result = result;
for (u32 k = 0; k < 2; k++)
	prepare_side(&self, k, task_verbose);
self.volume = 0;
self.gemm_usec = 0;
self.gemm_instr = 0;
self.gemm_cycles = 0;
self.part_usec = 0;
self.part_instr = 0;
self.part_cycles = 0;
self.subj_usec = 0;
self.subj_instr = 0;
self.subj_cycles = 0;


<<Release memory>>=
for (u32 k = 0; k < 2; k++) {
	free(L[k]);
	free(self.side[k].scratch);
	free(self.side[k].LM);
	free(self.side[k].count);
}
free(slices);


@ If requested, we are capable of displaying some information.

<<Display task info>>=
	/* task-level */
printf("Task: |A|=%" PRId64 ",  |B|=%" PRId64 "\n", n[0], n[1]);
double mbytes =  8 * (n[0] + n[1]) / 1048576.0;
printf("Volume. Hash = %.1fMbyte + Slice = %.1fMbyte\n", mbytes, 
						slices_size / 1048576.0);
double logsols = log2(1.5 * (n[0]) + log2(n[1]));
printf("Est. #solutions : %g\n", pow(2, logsols - 64));


@ Advancing to the next slice is tricky, because slices are variable-length.

<<Advance to next slice; [[break]] when done>>=
u32 n = slice->n;
u8 *ptr = (u8 *) slice;
ptr += sizeof(struct slice_t) + sizeof(u64) * n;
u8 *end = ((u8 *) slices) + slices_size;
if (ptr >= end)
	break;
slice = (struct slice_t *) ptr;


@ \section{Processing Slices}

We are ready for the big function. We compute the matrix $M$, then
for each pair $(x, y) \in AM \join BM$, we check whether $x ^ y \in CM$. Here,
$CM$ is very small. For now we get away with using a simple hash table with
linear probing.

Inside a slice, some parameters are fixed. The [[local_result]] object, the
matrix $M$, the ``join width'' $\ell$, the  hash table holding a slice of $C$,
etc.

<<Type definitions>>=
struct slice_ctx_t {
	const struct slice_t *slice;
	struct hash_table_t *H;
	struct task_result_t *result;	
};

<<Forward declarations>>=
void process_slice_v3(struct context_t *self, const struct slice_t *slice, 
								bool verbose);

<<Auxiliary functions>>=
void process_slice_v3(struct context_t *self, const struct slice_t *slice, 
								bool verbose)
{
	double start = wtime();
	<<Prepare slice context>>
	u32 fan_out = 1 << self->p;
	u64 probes = 0;
	u64 volume = self->n[0] + self->n[1];
	double Mvolume = volume * 9.5367431640625e-07;
	self->volume += volume;
	if (slice->l - self->p < 9)
		printf("WARNING : l and p are too close (increase l)\n");
	<<Compute $AM, BM$>>
	<<Partitioning>>
	<<Subjoins>>
	<<Lift solutions>>
	<<Destroy slice context>>
	if (verbose) {
		<<Display timing info>>
	}
}

<<Prepare slice context>>=
struct slice_ctx_t ctx = { .slice = slice };
ctx.result = result_init();
ctx.H = hashtable_build(slice->CM, 0, slice->n);

<<Compute $AM, BM$>>=
struct matmul_table_t M;
matmul_init(slice->M, &M);
// counters[0] = counters[1] = 0;
long long gemm_start = PAPI_get_real_usec();
long long instr = 0, cycles = 0;
#pragma omp parallel reduction(+:instr, cycles) num_threads(self->T_gemm)
{
	<<Read performance counters (start)>>
	gemm(self->L[0], self->side[0].LM, self->side[0].n, &M);
	gemm(self->L[1], self->side[1].LM, self->side[1].n, &M);
	<<Read performance counters (end)>>
//	printf("GEMM, tid=%d, intr=%lld, cycl=%lld\n", omp_get_thread_num(), instr, cycles);
}
self->gemm_usec += PAPI_get_real_usec() - gemm_start;
self->gemm_instr += instr;
self->gemm_cycles += cycles;
if (verbose) {
	double gemm_rate = Mvolume / (PAPI_get_real_usec() - gemm_start) * 1.048576;
	printf("[gemm/item] cycles = %.1f, instr = %.1f. Rate=%.1fMitem/s\n", 
		instr / Mvolume, cycles / Mvolume, gemm_rate); 
}


<<Partitioning>>=
long long part_start = PAPI_get_real_usec();
instr = 0, cycles = 0;
#pragma omp parallel reduction(+:instr, cycles) num_threads(self->T_part)
{
	<<Read performance counters (start)>>
	for (u32 k = 0; k < 2; k++)
		partition(self->p, &self->side[k]);
	<<Read performance counters (end)>>
}
self->part_usec += PAPI_get_real_usec() - part_start;
self->part_instr += instr;
self->part_cycles += cycles;
if (verbose) {
	double part_rate = Mvolume / (PAPI_get_real_usec() - part_start) * 1.048576;
	printf("[partition/item] cycles = %.1f, instr = %.1f. Rate=%.1fMitem/s\n", 
		instr / Mvolume, cycles / Mvolume, part_rate); 
}

<<Subjoins>>=
instr = 0, cycles = 0;
long long subj_start = PAPI_get_real_usec();
// printf("subj\n");
#pragma omp parallel reduction(+:probes, instr, cycles) num_threads(self->T_subj)
{
	<<Read performance counters (start)>>
	// u32 per_thread = ceil((1.0 * fan_out) / T);
	// u32 tid = omp_get_thread_num();
	// u32 lo = tid * per_thread;
	// u32 hi = MIN(fan_out, (tid + 1) * per_thread);
	#pragma omp for schedule(dynamic, 1)
	for (u32 i = 0; i < fan_out; i++) {
		<<Prepare subjoin input pointers for $i$-th partition>>
		probes += subjoin(&ctx, T, scattered);
	}
	<<Read performance counters (end)>>
}
self->subj_usec += PAPI_get_real_usec() - subj_start;
self->subj_instr += instr;
self->subj_cycles += cycles;
if (verbose) {
	double subjoin_rate = Mvolume / (PAPI_get_real_usec() - subj_start) * 1.048576;
	printf("[subjoin/item] Probes = %.4f, cycles = %.1f, instr = %.1f. Rate=%.1fMitem/s\n", 
		probes / Mvolume, instr / Mvolume, cycles / Mvolume, subjoin_rate); 
}


<<Destroy slice context>>=
hashtable_free(ctx.H);
result_free(ctx.result);


<<Lift solutions>>=
struct solution_t *loc = ctx.result->solutions;
u32 n_sols = ctx.result->size;
for (u32 i = 0; i < n_sols; i++) {
	u64 x = naive_gemv(loc[i].x, slice->Minv);
	u64 y = naive_gemv(loc[i].y, slice->Minv);
	u64 z = naive_gemv(loc[i].z, slice->Minv);
	report_solution(self->result, x, y, z);
}

<<Display timing info>>=
double duration = wtime() - start;
// printf("Block, total time: %.1fs\n", duration);
double volume = 9.5367431640625e-07 * (self->n[0] + self->n[1] + probes);
double rate = volume / duration;
printf("Join volume: %.1fM item (%.1fM item/s)\n", volume, rate);


@ Performance monitoring ! It seems that we cannot start and stop the counters 
arbitrarily often and/or too quickly. Therefore, we start them once, at the 
begining (in the caller module), and we will read them from time to time.

<<Read performance counters (start)>>=
long long counters[2] = {0, 0};
int rc;
if (false) {
	rc = PAPI_read_counters(counters, 2);
	if (rc < PAPI_OK)
		errx(1, "PAPI_read_counters (start): tid=%d, rc=%d, %s", omp_get_thread_num(), 
								rc, PAPI_strerror(rc));
	cycles = counters[0];
	instr = counters[1];
}

<<Read performance counters (end)>>=
if (false) {
	counters[0] = counters[1] = 0;
	rc = PAPI_read_counters(counters, 2);
	if (rc < PAPI_OK)
		errx(1, "PAPI_read_counters (end): tid=%d, rc=%d, %s", omp_get_thread_num(), 
								rc, PAPI_strerror(rc));
	cycles = counters[0] - cycles;
	instr = counters[1] - instr;
}

@ The technical part consists in computing $AM \join_\ell BM$. Several methods
are possible. We explore the complications below, incrementally. For now, let
us assume that the join has been computed, that solutions have been found, and
let us deal with the aftermath.

In [[local_result]], we have ``solutions'' of the $(AM, BM, CM)$ instance, i.e. 
triplets $(xM, yM, zM)$, so we multiply them by $M^{-1}$ to get the actual 
solutions.

Because there are few such multiplications (we expect roughly $\ell$), we use
a naive vector-matrix product.

<<Forward declarations>>=
u64 naive_gemv(u64 x, const u64 *M);

<<Auxiliary functions>>=
u64 naive_gemv(u64 x, const u64 *M)
{
	u64 y = 0;
	for (u32 i = 0; i < 64; i++) {
		u64 bit = (x >> i) & 1;
		u64 mask = (u64) (- ((i64) bit));
		y ^= M[i] & mask;
	}
	return y;
}

@ \section{Matrix product}

We first do the matrix product, then the join. To perform the fast matrix-
matrix product, we use a precomputed table inspired by the four russians
algorithm.

In our datastructure, [[table[i]]] contains all the 256 possible linear combinations
of rows $[8i: 8(i+1)]$. We use a gray code to compute these more efficiently. At the 
$j$-th step, we XOR the $k$-th row of the block of 8, where $k$ is the $2$-valuation of $j$.
The $j$-th word in gray code order is [[j ^ (j >> 1)]].

<<Type definitions>>=
struct matmul_table_t {
	u64 tables[8][256] __attribute__((aligned(64)));
};

<<Auxiliary functions>>=
static inline u64 gemv(u64 x, const struct matmul_table_t * M)
{
	u64 r = 0;
	r ^= M->tables[0][x & 0x00ff];
	r ^= M->tables[1][(x >> 8) & 0x00ff];
	r ^= M->tables[2][(x >> 16) & 0x00ff];
	r ^= M->tables[3][(x >> 24) & 0x00ff];
	r ^= M->tables[4][(x >> 32) & 0x00ff];
	r ^= M->tables[5][(x >> 40) & 0x00ff];
	r ^= M->tables[6][(x >> 48) & 0x00ff];
	r ^= M->tables[7][(x >> 56) & 0x00ff];
	return r;
}

<<Auxiliary functions>>=
void matmul_init(const u64 *M, struct matmul_table_t* T)
{
	#pragma omp parallel for schedule(static)
	for (u32 i = 0; i < 8; i++) {
		u32 lo = i * 8;
		T->tables[i][0] = 0;
		u64 tmp = 0;
		for (u32 j = 1; j < 256; j++) {
			u32 k =  ffs(j) - 1;
			tmp ^= M[lo + k];
			T->tables[i][j ^ (j >> 1)] = tmp;
		}
	}
}

<<Auxiliary functions>>=
static inline void gemm(const u64 *IN, u64 *OUT, u32 n, const struct matmul_table_t *M)
{
	#pragma omp for schedule(static)
	for (u32 i = 0; i < n; i++)
		OUT[i] = gemv(IN[i], M);
}


<<Forward declarations>>=
static inline u64 gemv(u64 x, const struct matmul_table_t * M);
void matmul_init(const u64 *M, struct matmul_table_t* T);
static inline void gemm(const u64 *IN, u64 *OUT, u32 n, const struct matmul_table_t *M);

@ Now we have everything we need to do the linear algebra.


@ \section{Partitioning}

The partitionning function is meant to be executed by all threads. Upon return,
the [[count]] parameter reveals the extent of each thread-bucket. It is
guaranteed  that an \OMP barrier happens before this function returns (the
[[for]] directive does this).

<<Forward declarations>>=
void partition(u32 p, struct side_t *side);

<<Auxiliary functions>>=
void partition(u32 p, struct side_t *side)
{
	u32 tid = omp_get_thread_num();
	u32 fan_out = 1 << p;
	u32 *count = side->count + tid * fan_out;
	for (u32 i = 0; i < fan_out; i++)
		count[i] = side->psize * i + side->tsize * tid;
	const u64 *L = side->LM;
	const u32 n = side->n;
	u64 *scratch = side->scratch;
	u8 shift = 64 - p;
	// u32 per_thread = 1 + n / T;
	// u32 lo = tid * per_thread;
	// u32 hi = MIN(n, (tid + 1) * per_thread);
	#pragma omp for schedule(static)
	for (u32 i = 0; i < n; i++) {
		u64 x = L[i];
		u64 h = x >> shift;
		// assert(h < fan_out);
		u32 idx = count[h]++;
		scratch[idx] = x;
	}
}

@ \section{Sub-Joins}

The result of the partitioning step is scattered in $T$ places. These 
locations are non-contiguous. To decouple subsequent processing from this 
mess, we prepare a \emph{list} of $T$ inputs.

<<Type definitions>>=
struct scattered_t {
	u64 **L;
	u32 *n;
};


<<Prepare subjoin input pointers for $i$-th partition>>=
u32 T = self->T_part;
u64 *L[2][T];
u32 n[2][T];
struct scattered_t scattered[2];
for (u32 k = 0;  k < 2; k++) {
	scattered[k].L = L[k];
	scattered[k].n = n[k];
	struct side_t *side = &self->side[k];
	for (u32 t = 0; t < T; t++) {
		u32 lo = side->psize * i + side->tsize * t;
		u32 hi = side->count[t * fan_out + i];
		scattered[k].L[t] = side->scratch + lo;
		scattered[k].n[t] = hi - lo;
	}
}

@ The partitions are small. We use our mighty hash table with linear probing. We
want the hash table to have a  fixed size, compatible with that of the lowest
level of cache.

<<Forward declarations>>=
u64 subjoin(struct slice_ctx_t *ctx, u32 T, struct scattered_t *partitions);

<<Auxiliary functions>>=
u64 subjoin(struct slice_ctx_t *ctx, u32 T, struct scattered_t *partitions)
{
	static const u32 HASH_SIZE = 16384  / 4 / sizeof(u64);
	static const u64 HASH_MASK = 16384 / 4 / sizeof(u64) - 1;
	u8 l = ctx->slice->l;
	u8 shift = 64 - l;
	<<Prepare hash table [[H]]>>
	// u32 build_probes = 0;
	// u32 build_volume = 0;
	for (u32 t = 0; t < T; t++) {
		<<Build the hash table using $A$>>
		// build_volume += nA;
	}
	// u32 probe_probes = 0;
	// u32 probe_volume = 0;
	for (u32 t = 0; t < T; t++) {
		<<Probe the hash table with $B$>>
		// probe_volume += nB;
	}
	// printf("[subjoin] mems/item [build] = %.2f, mems/item [probe] = %.2f\n",
	// 	(1.0 * build_probes) / build_volume,
	// 	(1.0 * probe_probes) / probe_volume);
	return emitted;
}

<<Prepare hash table [[H]]>>=
u64 emitted = 0;
u64 H[HASH_SIZE];
for (u32 i = 0; i < HASH_SIZE; i++)
	H[i] = 0;

<<Build the hash table using $A$>>=
u64 *A = partitions[0].L[t];
u32 nA = partitions[0].n[t];
for (u32 i = 0; i < nA; i++) {
	u32 h = (A[i] >> shift) & HASH_MASK;
	// build_probes++;
	while (H[h] != 0) {
		h = (h + 1) & HASH_MASK;
		// build_probes++;
	}
	H[h] = A[i];
}

<<Probe the hash table with $B$>>=
u64 *B = partitions[1].L[t];
u32 nB = partitions[1].n[t];
for (u32 i = 0; i < nB; i++) {
	u64 y = B[i];
	u32 h = (y >> shift) & HASH_MASK;
	u64 x = H[h];
	//probe_probes++;
	while (x != 0) {
		u64 z = x ^ y;
		if ((z >> shift) == 0) {
			// printf("Trying %016" PRIx64 " ^ %016" PRIx64 " ^ %016" PRIx64 "\n", x, y, z);
			<<Emit $(x, y)$>>
		}
		h = (h + 1) & HASH_MASK;
		x = H[h];
		//probe_probes++;
	}
}

<<Emit $(x, y)$>>=
if (hashtable_lookup(ctx->H, z))
	report_solution(ctx->result, x, y, z);
emitted++;

@ \section{speed}

41s/task, 1 thread
28s/task, 2 thread
26.4s/task, 3 thread
28s/task, 4 thread


Task duration: 40.8 s
Breakdown: (T = 1)
* GEMM:      	time = 18.25s	IPC = 0.52	instr/item = 33.6	cycles/item = 64.5
* partition: 	time = 7.16s	IPC = 0.55	instr/item = 14.1	cycles/item = 25.7
* subjoin:   	time = 14.85s	IPC = 0.47	instr/item = 25.2	cycles/item = 54.0

Task duration: 28.2 s
Breakdown: (T = 2)
* GEMM:      	time = 11.35s	IPC = 0.84	instr/item = 33.4	cycles/item = 80.4
* partition: 	time = 4.84s	IPC = 0.82	instr/item = 14.2	cycles/item = 34.4
* subjoin:   	time = 11.38s	IPC = 0.64	instr/item = 26.4	cycles/item = 82.2

Task duration: 26.2 s
Breakdown: (T = 3)
* GEMM:      	time = 10.02s	IPC = 0.93	instr/item = 33.5	cycles/item = 107.2
* partition: 	time = 5.33s	IPC = 0.75	instr/item = 14.3	cycles/item = 56.5
* subjoin:   	time = 10.50s	IPC = 0.72	instr/item = 26.6	cycles/item = 112.8

Task duration: 27.9 s
Breakdown: (T = 4)
* GEMM:      	time = 9.96s	IPC = 0.96	instr/item = 33.6	cycles/item = 140.0
* partition: 	time = 7.72s	IPC = 0.52	instr/item = 14.4	cycles/item = 109.2
* subjoin:   	time = 9.98s	IPC = 0.76	instr/item = 27.0	cycles/item = 145.1


partition: avec bcp de SMT-threads, chacun a moins de cache. Donc c'est normal 
qu'on observe une dégradation !

subjoin : peut-être paralléliser la phase de probe ? c.a.d. 2 subjoin en //, 1 
thread pour le build et 2 threads pour le probe ? matérialiser le join 
(dans chaque probe thread) ?

-------

 Breakdown:
stdout[12]: * GEMM:      	T = 4, 	time = 9.79s	rate = 47.02Mitem/s
stdout[12]: * partition: 	T = 2, 	time = 6.14s	rate = 74.91Mitem/s
  ------> bw = 13 Gb/s read + as much write.
stdout[12]: * subjoin:   	T = 3, 	time = 9.23s	rate = 49.85Mitem/s

stdout[14]: * GEMM:      	T = 4, 	time = 9.95s	rate = 46.24Mitem/s
stdout[14]: * partition: 	T = 4, 	time = 7.17s	rate = 64.21Mitem/s
stdout[14]: * subjoin:   	T = 4, 	time = 9.81s	rate = 46.90Mitem/s

En l'état, un peu moins de 8 millions d'heures sur turing.

partition = existee-t-il compare-and-swap ? atomic increment ? Ceci permettrait
de faire marcher 4 threads de partitionnement sur les mêmes buckets de sortie.

thread speculation ?

@ \end{document}