\documentclass{book}

\usepackage{geometry}
\usepackage{noweb}
\usepackage{amsmath,amsfonts,amssymb}
\usepackage{parskip}
\usepackage{xspace}
\usepackage{hyperref}

\newcommand{\join}{\bowtie}

\begin{document}

\setcounter{chapter}{2}
\chapter{The Iterated Joux Algorithm}


This file describes an implementation of the iterated Joux algorithm.  
The main function described in this file ``solves'' a (fine) task. 

<<*>>=
#include <math.h>
#include <stdlib.h>
#include <stdio.h>
#include <assert.h>
#include <err.h>
#include <sys/types.h>
#include <sys/stat.h>

#include "common.h"
#include "linalg.h"
#include "../quadratic/datastructures.h"

<<Type definitions>>
<<Forward declarations>>
<<Auxiliary functions>>
<<The main function>>

@ \section{Processing Tasks}

<<The main function>>=
struct task_result_t * iterated_joux_task(const char *hash_dir, struct jtask_id_t *task)
{
	static const bool verbose = true;
	struct task_result_t *result = result_init();
	double start = wtime();
	<<Load data from disk>>
	<<Prepare context>>
	if (verbose) {
		<<Display task info>>
	}
	for (u32 u = 0; u < n_blocks; u++)
		process_block(&self, u, false);
	<<Release memory>>
	if (verbose) {
		<<Display task timing>>
	}
	return result;
}


@ \section{Preparations}

Preparations include loading the data from disk. We expected a fair amount of 
data (say, 1Gb). Therefore this may take some time. In the future, we would
use this time to do something useful (try to precompute some matrices ?).

<<Load data from disk>>=
u64 *slice[3];
u64 *scratch[3];
u32 size[3];
for (u32 kind = 0;  kind < 3; kind++) {	
	<<Determine [[filename]] and Query file size>>
	<<Allocate memory>>
	<<Open file and load hashes>>
	<<Allocate scratch space>>
}

@ The following code is NOT DRY wrt quadratic. If we are dealing with $C$, we
ajust the offset and size (as given by the task descriptor).

<<Determine [[filename]] and Query file size>>=
char filename[255];
char *kind_name[3] = {"foo", "bar", "foobar"};
sprintf(filename, "%s/%s.%03x", hash_dir, kind_name[kind], task->idx[kind]);
struct stat infos;
if (stat(filename, &infos))
	err(1, "fstat failed on %s", filename);
assert((infos.st_size % 8) == 0);
u64 m = infos.st_size / sizeof(u64) / task->k2;
u64 lo = 0;
u64 hi = infos.st_size;
if (kind == 2) {
	lo = 8 * (task->r * m);
	hi = MIN(hi, 8 * ((task->r  + 1) * m));
}
assert (lo != hi);

@ When allocating the memory, we enforce alignement on a 64-byte boundary 
(the size of a cache line on most CPUs). In addition, this allows aligned 
access for all possible sizes, including 256-bit registers.
For this, we use [[aligned_alloc]], available in \textsf{C11}.

<<Allocate memory>>=
u64 aligned_size = 64 * ceil((hi - lo) / 64.);
slice[kind] = aligned_alloc(64, aligned_size);
if (slice[kind] == NULL)
	err(1, "failed to allocate memory");
size[kind] = (hi - lo) / sizeof(u64);

<<Open file and load hashes>>=
FILE *f = fopen(filename, "r");
if (f == NULL)
	err(1, "fopen failed (%s)", filename);
fseek(f, lo, SEEK_SET);
u64 check = fread(slice[kind], 1, hi - lo, f);
if ((check != hi - lo) || ferror(f))
	err(1, "fread : read %" PRId64 ", expected %zd", check, hi - lo);
if (fclose(f))
	err(1, "fclose %s", filename);

<<Allocate scratch space>>=
scratch[kind] = aligned_alloc(64, aligned_size);
if (scratch[kind] == NULL)
	err(1, "failed to allocate memory");

<<Release memory>>=
for (u32 kind = 0;  kind < 3; kind++) {
	free(slice[kind]);
	free(scratch[kind]);
}


@ The task is split into blocks by splitting the $C$ list into very small 
slices of size $64 - \ell$. The optimal value of $\ell$ is $\max (\log_2 A, 
\log_2 B)$.

<<Type definitions>>=
struct context_t {
	struct task_result_t *result;
	u64 *slice[3];
	u64 *scratch[3];
	u32 size[3];
	u32 l;
};

<<Prepare context>>=
struct context_t self;
self.result = result;
for (u32 i = 0; i < 3; i++) {
	self.size[i] = size[i];
	self.slice[i] = slice[i];
	self.scratch[i] = scratch[i];
}
self.l = ceil(MAX(log2(size[0]), log2(size[1])));
u32 n_blocks = ceil(size[2] / (64. - self.l));



@ If requested, we are capable of displaying some information.

<<Display task info>>=
	/* task-level */
printf("Task: |A|=%d,  |B|=%d,  |C|=%d\n", size[0], size[1], size[2]);
double mbytes = 8. * (size[0] + size[1] + size[2]) / 1048576.0;
printf("Task volume : %.1fMbyte of hashes\n", mbytes);
double logsols = log2(size[0]) + log2(size[1]) + log2(size[2]);
printf("Est. #solutions : %g\n", pow(2, logsols - 64));
	/* block-level */
printf("Using l = %d\n", self.l);
printf("#blocks = %d\n", n_blocks);


<<Display task timing>>=
double task_duration = wtime() - start;
u32 n_coarse_tasks = 1 << (2 * task->k);
double total = task_duration * n_coarse_tasks * task->k2;
printf("Task duration: %.1f s\n", task_duration);
printf("Est. total time: %.3e h\n", total / 3600);


@ \section{Processing Blocks}

We define a simple auxiliary function to perform a matrix product.

We are ready for the big function. We compute the matrix $M$, then
for each pair $(x, y) \in AM \join BM$, we check whether $x ^ y \in CM$. Here,
$CM$ is very small. For now we get away with using a simple hash table with
linear probing.

<<Forward declarations>>=
void process_block_v0(struct context_t *self, u32 u, bool verbose);

<<Auxiliary functions>>=
void process_block_v0(struct context_t *self, u32 u, bool verbose)
{
	double start = wtime();
	struct task_result_t * local_result = result_init();
	<<Identify slice of $C$>>
	if (verbose) {
		<<Display stuff>>
	}
	<<Compute change of basis matrix>>
	<<Compute $AM, BM$ and $CM$>>
	<<Sort $AM$ and $BM$>>
	<<Build hash table>>
	<<Join $AM$ and $BM$ on $\ell$ bits>>
	<<Backport solutions>>
	<<Deallocate hash table>>
	if (verbose) {
		<<Display timing info>>
	}
}

@ First, in all cases we will have to compute the change-of-basis matrix.

\subsection{Linear Algebra}

<<Auxiliary functions>>=
void gemm(const u64 *IN, u64 *OUT, u32 n, const struct matmul_table_t *M)
{
	for (u32 i = 0; i < n; i++)
		OUT[i] = gemv(IN[i], M);
}


<<Identify slice of $C$>>=
u32 size[3];
for (u32 i = 0; i < 3; i++)
	size[i] = self->size[i];
u32 l = self->l;
u32 slice_height = 64 - l;
u32 lo = u * slice_height;
u32 hi = MIN((u + 1) * slice_height, size[2]);
const u64 *C = self->slice[2] + lo;
slice_height = hi - lo;

<<Display stuff>>=
printf("Block %d: C[%d:%d] (slice_height=%d)\n", u, lo, hi, hi - lo);

<<Compute change of basis matrix>>=
struct matmul_table_t M, M_inv;
u32 rank;
#pragma omp critical(M4RI_is_not_bloody_thread_safe)
rank = basis_change_matrix(C, 0, slice_height, &M, &M_inv);
if (rank != slice_height && verbose)
	printf("---> Rank deffect (%d)\n", slice_height - rank);
l = 64 - rank;

@ The technical part consists in computing $AM \join_\ell BM$. Several methods
are possible. We explore the complications below, incrementally. For now, let
us assume that the join has been computed, that solutions have been found, and
let us deal with the aftermath.

In [[local_result]], we have ``solutions'' of the $(AM, BM, CM)$ instance, i.e. 
triplets $(xM, yM, zM)$, so we multiply them by $M^{-1}$ to get the actual 
solutions.

<<Backport solutions>>=
struct solution_t *loc = local_result->solutions;
u32 n_sols = local_result->size;
for (u32 i = 0; i < n_sols; i++) {
	u64 x = gemv(loc[i].x, &M_inv);
	u64 y = gemv(loc[i].y, &M_inv);
	u64 z = gemv(loc[i].z, &M_inv);
	report_solution(self->result, x, y, z);
}

<<Display timing info>>=
double duration = wtime() - start;
printf("Block %d, total time: %.1fs\n", u, duration);
double volume = 9.5367431640625e-07 * (size[0] + size[1] + probes);
double rate = volume / duration;
printf("Join volume: %.1fM item (%.1fM item/s)\n", volume, rate);

@ \section{Join v0: naive matrix product and (very) naive sort-join}

What is described next is a naive, direct implementation.

<<Compute $AM, BM$ and $CM$>>=
u64 *AM = self->scratch[0];
u64 *BM = self->scratch[1];
u64 *CM = self->scratch[2];
gemm(self->slice[0], AM, size[0], &M);
gemm(self->slice[1], BM, size[1], &M);
gemm(C, CM, slice_height, &M);

@ For now, we use the stupidest possible sort.

<<Forward declarations>>=
int cmp(const void *a_, const void *b_);

<<Auxiliary functions>>=
int cmp(const void *a_, const void *b_)
{
	u64 *a = (u64 *) a_;
	u64 *b = (u64 *) b_;
	return (*a > *b) - (*a < *b);
}

<<Sort $AM$ and $BM$>>=
qsort(AM, size[0], sizeof(u64), cmp);
qsort(BM, size[1], sizeof(u64), cmp);

<<Build hash table>>=
struct hash_table_t *D = hashtable_build(CM, 0, slice_height);

<<Deallocate hash table>>=
hashtable_free(D);

<<Join $AM$ and $BM$ on $\ell$ bits>>=
u64 a = 0;
u64 b = 0;
u64 mask = LEFT_MASK(l);
u64 probes = 0;
for (u32 i = 0; i < slice_height; i++)
	assert((CM[i] & mask) == 0);
while (a < size[0] && b < size[1]) {		
	u64 prefix_a = AM[a] & mask;
	u64 prefix_b = BM[b] & mask;
	if (prefix_a < prefix_b) {
		a += 1;
		continue;
	}
	if (prefix_a > prefix_b) {
		b += 1;
		continue;
	}
	/* Here, prefix_a == prefix_b */
	u64 b_0 = b;
	while (1) {
		probes++;
		u64 z = AM[a] ^ BM[b];
		if (hashtable_lookup(D, z))
			report_solution(local_result, AM[a], BM[b], z);
		b += 1;
		if (b < size[1] && (BM[b] & mask) == prefix_b)
			continue;
		a += 1;
		if (a < size[0] && (AM[a] & mask) == prefix_a) {
			b = b_0;
			continue;
		}
		break;
	}
}

@ \section{Join v1: Partitioning Join}

We use a \emph{partitioning} (hash) join. To compute $A \join B$, we divide 
$A$ and $B$ into partitions $(A_i)$ and $(B_j)$ such that
$A_i \join B_j = \emptyset$ when $i \neq j$. Radix partitioning puts in $A_i$
all entries from $A$ that begin with $i$ (the $\ell$ most-significant bits
of each key in $A_i$ is $i$). If we divide $A$ using $\ell$ key bits, we end up
with $2^p$ partitions. The first partitionning step must be multi-threaded.
Then all sub-joins can be processed in parallel, each inside a single thread.

The usual way to do radix-partitioning does two passes over the input: a first
pass counts the number of elements in each output bucket, then a second pass
actually dispatches the data.

We choose to do away with the first pass at the expense of a small increase in
storage. Each one of the $T$ threads directly dispatches its slice of the input
into $2^p$ \emph{thread-private} buckets. Each partition $A_i$ is then
composed of $T$ such thread-buckets. An added bonus is that it allows us to do
the matrix multiplication on the fly (during the partitioning step).

The drawback of this approach is that the size of each thread-bucket is not
known in advance. Because the keys are random, we may safely overestimate it
though. If $N$ denotes the size of the input list, then each output bucket 
receives on average $\mu = N / (2^p T)$ entries. A Chernoff bound tells 
us that if $X$ denotes an actual thread-bucket size,
\[
\mathbb{P}\left[X > (1 + \delta)\mu \right] < \left( \frac{e^\delta}{(1+\delta)^{1+\delta}}\right)^\mu.
\]

We want to choose a safety margin $\delta$ such that the possibility of overflow
is remote (say $2^{-100}$). It is then enough to choose
$\delta \gets \sqrt{210 / \mu}$ (this is a loose bound). As long as $N$ is 
large enough, a small $\delta$ (20\% or less) is sufficient, and the space 
overhead is limited.

Given an input array [[IN]] of size [[N]], we will therefore allocate an array 
[[OUT]] of size $O \gets \left\lceil(1 + \delta)N\right\rceil$. This output 
array is split into $2^p$ ``prefix-zones'' of size $[[psize]] = [[output_size]] / 2^p$.
Each prefix-zone is itself split into $T$ ``thread-zone'' of size
$[[tsize]] = [[psize]] / T$. Each thread writes the entries it reads from the
input into his own thread-zones in all prefix-zones. We ensure that the
``thread-zones'' have a size which is a multiple of  [[CACHE_LINE_SIZE]]. This
large memory allocation can be done once and for all during the processing of
a task.

FIX FIX FIX

To get the number of threads, we open an \OMP 
parallel section, then require that a single thread does the memory 
allocation.

@ We want [[OUT]] to be aligned on a cache line boundary, so we use an ad hoc
allocation mecanism. TODO : reformuler avec [[aligned_alloc]].

<<Allocate [[OUT]] and [[COUNT]]>>=
int rc = posix_memalign((void **) OUT, CACHE_LINE_SIZE, output_size * sizeof(u64));
if (!rc)
	err(1, "aligned memory allocation");
rc = posix_memalign((void **) COUNT, CACHE_LINE_SIZE, T * fan_out * sizeof(int));
if (!rc)
	err(1, "aligned memory allocation");


<<definitions>>=
static const u32 CACHE_LINE_SIZE = 64;

<<Compute output size>>=
u32 fan_out = 1 << p;
double mu = N / (fan_out * T);
double delta = sqrt(210 / mu);
double tmp = ceil(N * (1 + delta) / fan_out / T / CACHE_LINE_SIZE);
u32 tsize = tmp * CACHE_LINE_SIZE;
u32 psize = tsize * T;
u64 output_size = psize * fan_out;

@ The partitionning function is meant to be executed by all threads. It 
provides the number of elements in each thread-bucket. This goes into the 
[[COUNT]] array: the number of items with key $i$ observed by thread $t$ is 
[[COUNT[t*fan_out + i]]]. 

In the partition function below, [[OUT]] must be preallocated of size 
[[output_size]], and [[COUNT]] must be preallocated of size [[T * fan_out]]. 
The number of threads and all other variables will be passed in the ``context''
object ([[self]]). The function is meant to be called inside an \OMP parallel
region.

<<Functions>>=
void stage1_partition(const struct context_t *self, const u64 *IN, int N, u64 *OUT, u32 *COUNT, const struct matmul_table_t * M)
{ 
	u32 t = tid();
	u32 fan_out = self->fan_out;
	u32 psize = self->psize;
	u32 tsize = self->tsize;
	u32 *tCOUNT = COUNT + t * fan_out;
	for (u32 i = 0; i < fan_out; i++)
		tCOUNT[i] = psize * i + tsize * t;
	/* dispatch */
	#pragma omp for schedule(static)
	for (int i = 0; i < N; i++) {
		u64 x = gemv(IN[i], M);
		u32 h = x >> (64 - l);
		u32 idx =  tCOUNT[i]++;
		OUT[idx] = x;
	}
}


@ The fan-out has a large influence on the speed of dispatching. If the fan-out
exceeds the number of cache lines, then writing to the output is likely to cause
a cache miss. In addition, if the fan-out if higher than the number of TLB
entries, then each write may cause a page walk. On most currently available
CPUs, the L1 cache has 512 cache lines (shared between several hardware contexts),
the level-1 TLB has 32/64 entries and the level-2 TLB has 512 entries.

In practice, one observes a severe degradation when the fan-out is greater than
256 or 512. The TLB effect can be partially alleviated thanks to a neat
implementation trick: the \emph{software write-combining buffer}. (LATER).




<<Allocate and initialize buffer write-combining buffer>>=
struct cacheline_t *buffer;
int rc = posix_memalign((void **) &buffer, CACHE_LINE_SIZE, sizeof(struct cacheline_t) * fan_out);
if (rc != 0)
	err(1, "aligned allocation of write-combining buffer");
for (int i = 0; i < fan_out; i++ )
	buffer[i].slot = *OUT + psize * i + tsize * t;


<<Load |IN[i]| into the write-combining buffer...>>=
int slot_mod = slot & 7;
buffer[h].value[slot_mod] = x;
if (slot_mod == 7)
	store_nontemp_64B(*OUT + (slot - 7), buffer + h);
buffer[h].slot = slot + 1;        

<<Flush the write-combining buffer>>=
for (int i = 0; i < fan_out; i++) {
	int64_t slot = buffer[i].data.slot;
        tCOUNT[i] = slot;
	int sz = slot & 7;
	slot -= sz;
	for (int j = 0; j < sz; j++)
    		OUT[slot++] = buffer[i].data.values[j];
}


Dispatching can be done in-place, but it is slower and more complicated. We use
parallel counting and dispatching. During the counting phase, each thread gets a
slice of the input list, and counts the number of occurences of each key in its
slice. Using $k$ key bits, and $T$ threads, this results in $T$ count arrays
\texttt{TCOUNT[0:T][0:$2^k$]}, where |TCOUNT[t][i]| is the number of input items
with key $i$ observed by thread $t$.

\begin{tikzpicture}[>=latex]
  \draw[thick,dotted] (0, 0) -- (0, -1);
  \draw[thick,dotted] (3, 0) -- (3, -1);
  \draw[thick] (0, 0) -- (0, 8);
  \draw[thick] (3, 0) -- (3, 8);
  \draw[thick] (0, 8) -- (3, 8);
  \draw (0, 6) -- +(3, 0);
  \draw (0, 4) -- +(3, 0);
  \draw (0, 2) -- +(3, 0);
  \draw (0, 0) -- +(3, 0);
  \path (0, 8) rectangle node {Thread 0} +(3, -2);
  \path (0, 6) rectangle node {Thread 1} +(3, -2);
  \path (0, 4) rectangle node {Thread 2} +(3, -2);
  \path (0, 2) rectangle node {Thread 3} +(3, -2);

  \newlength{\espacehoriz}
  \setlength{\espacehoriz}{10cm}
  
  
  \begin{scope}[xshift=\espacehoriz,font=\footnotesize]
    \draw[thick,dotted] (0, 0) -- (0, -1);
    \draw[thick,dotted] (3, 0) -- (3, -1);
    \draw[thick] (0, 0) -- (0, 8);
    \draw[thick] (3, 0) -- (3, 8);
    \draw[thick] (0, 8) -- (3, 8);
    
    \draw (0, 5) -- +(3, 0);
    \path (0, 5) rectangle node[font=\normalsize] {Partition 0} +(3, +3);

    \foreach \y in {7.5, 7.2, 6.6, 6.2}
      \draw[thick] (-0.05, \y) -- +(0.1, 0) (2.95, \y) -- +(0.1, 0);
    
    \path (3, 8)   node[right] {|TCOUNT[0][0] = COUNT[0]|};
    \path (3, 7.5) node[right] {|TCOUNT[1][0]|};
    \path (3, 7.2) node[right] {|TCOUNT[2][0]|};
    \path (3, 6.6) node[right] {|TCOUNT[3][0]|};
    \path (3, 6.2) node[right] {|TCOUNT[4][0]|};

    \draw (0, 4) -- +(3, 0);
    \path (0, 4) rectangle node[font=\normalsize] {Partition 1} +(3, +1);

    \foreach \y in {4.2, 4.4, 4.6, 4.8}
      \draw[thick] (-0.05, \y) -- +(0.1, 0) (2.95, \y) -- +(0.1, 0);

    \path (3, 5) node[right] {|TCOUNT[0][1]| = |COUNT[1]|};    
    
    \draw (0, 2) -- +(3, 0);
    \path (0, 2) rectangle node[font=\normalsize] {Partition 2} +(3, +2);
    \path (3, 4) node[right] {|TCOUNT[0][2]| = |COUNT[2]|};
    \path (3, 3.5) node[right] {|TCOUNT[1][2]|};
    \path (3, 3.2) node[right] {|TCOUNT[2][2]|};
    \path (3, 2.6) node[right] {|TCOUNT[3][2]|};
    \path (3, 2.2) node[right] {|TCOUNT[4][2]|};

    \foreach \y in {3.5, 3.2, 2.6, 2.2}
      \draw[thick] (-0.05, \y) -- +(0.1, 0) (2.95, \y) -- +(0.1, 0);

    \draw (0, 2) -- +(3, 0);
    \path (0, 0.5) rectangle node[font=\normalsize] {Partition 3} +(3, +1.5);
    \draw (0, 0.5) -- +(3, 0);

    \foreach \y in {1.6, 1.3, 0.8, 0.6}
      \draw[thick] (-0.05, \y) -- +(0.1, 0) (2.95, \y) -- +(0.1, 0);
  \end{scope}

  \begin{scope}[shorten >=1mm]
  \draw[->] (3, 7) -- (\espacehoriz, 7.75);
  \draw[->] (3, 5) -- (\espacehoriz, 7.35);
  \draw[->] (3, 3) -- (\espacehoriz, 6.9);
  \draw[->] (3, 1) -- (\espacehoriz, 6.4);

  \draw[->] (3, 7) -- (\espacehoriz, 4.8);
  \draw[->] (3, 5) -- (\espacehoriz, 4.6);
  \draw[->] (3, 3) -- (\espacehoriz, 4.4);
  \draw[->] (3, 1) -- (\espacehoriz, 4.2);

  \draw[->] (3, 7) -- (\espacehoriz, 3.75);
  \draw[->] (3, 5) -- (\espacehoriz, 3.35);
  \draw[->] (3, 3) -- (\espacehoriz, 2.9);
  \draw[->] (3, 1) -- (\espacehoriz, 2.4);

  \draw[->] (3, 7) -- (\espacehoriz, 1.8);
  \draw[->] (3, 5) -- (\espacehoriz, 1.55);
  \draw[->] (3, 3) -- (\espacehoriz, 1.05);
  \draw[->] (3, 1) -- (\espacehoriz, 0.7);

\end{scope}
\end{tikzpicture}




@ \end{document}