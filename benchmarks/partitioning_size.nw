\documentclass{book}

\usepackage{geometry}
\usepackage{noweb}
\usepackage{amsmath,amsfonts,amssymb}
\usepackage{parskip}
\usepackage{xspace}
\usepackage{hyperref}
\usepackage{tikz}
\usetikzlibrary{patterns}

\newcommand{\join}{\bowtie}
\newcommand{\OMP}{\textsf{OpenMP}\xspace}

\begin{document}

\chapter{Partitioning Benchmark}

Here, we study how the size of the input data influences partitioning speed.

Let's start will all the useless stuff.

<<*>>=
#define _XOPEN_SOURCE
#include <math.h>
#include <stdlib.h>
#include <stdio.h>
#include <assert.h>
#include <err.h>
#include <sys/types.h>
#include <sys/stat.h>
#include <omp.h>
#include <getopt.h>
#include <immintrin.h>

#include "common.h"


<<Settings>>
<<Type definitions>>
<<Function prototypes>>
<<Auxiliary functions>>
<<The main function>>

<<Settings>>=
#define check false
#define update false
#define product true



<<The main function>>=
int main(int argc, char **argv)
{
	if (check)
		warnx("CHECK IS ON");
	if (update)
		warnx("UPDATE IS ON");
	if (product)
		warnx("PRODUCT IS ON");
	<<Process the command line>>
	<<Prepare input data>>
	<<Run everything>>
}


<<Process the command line>>=
struct option longopts[4] = {
	{"n", required_argument, NULL, 'n'},
	{"it", required_argument, NULL, 'i'},
	{NULL, 0, NULL, 0}
};
u64 maxn = 2049;  /* in Koctabytes */
u32 iterations = 20;
char ch;
while ((ch = getopt_long(argc, argv, "", longopts, NULL)) != -1) {
    	switch (ch) {
	case 'n':
		maxn = atoll(optarg);
		break;
	case 'i':
		iterations = atoi(optarg);
		break;
	default:
		errx(1, "Unknown option\n");
	}
}

<<Auxiliary functions>>=
static const u32 CACHE_LINE_SIZE = 64;
#define p 8

u64 ROUND(u64 s)
{
	return CACHE_LINE_SIZE * ceil(((double) s) / CACHE_LINE_SIZE);
}

u64 chernoff_bound(u64 N, u32 n_buckets)
{
	double mu = ((double) N) / n_buckets;
	double delta = sqrt(210 / mu);
	return ROUND(N * (1 + delta) / n_buckets);
}



<<Prepare input data>>=
printf("# Loading random junk\n");
u32 n = maxn * 1024;
u64 *IN = aligned_alloc(64, sizeof(*IN) * ROUND(n));
if (IN == NULL)
	err(1, "cannot allocate memory");
FILE *f = fopen("/dev/urandom", "r");
u64 read = fread(IN, sizeof(*IN), n, f);
if (read != n)
	err(1, "cannot read random junk");
fclose(f);


<<Prepare scratch space>>=
u8 shift = 64 - p;
u32 fan_out = 1 << p;
u32 tsize = chernoff_bound(n, T * fan_out);
u32 psize = tsize * T;
u32 scratch_size = psize * fan_out;
u64 *OUT = aligned_alloc(64, sizeof(*OUT) * scratch_size);
u32 *COUNT = aligned_alloc(64, ROUND(sizeof(*COUNT) * fan_out * T));
if (OUT == NULL || COUNT == NULL)
	err(1, "cannot allocate memory");


<<Prepare random matrix>>=
struct matmul_table_t M;
for (u32 i = 0; i < 8; i++)
	for (u32 j = 0; j < 256; j++)
	M.tables[i][j] = mrand48() ^ (((u64) mrand48()) << 32);


<<Type definitions>>=
struct matmul_table_t {
	u64 tables[8][256] __attribute__((aligned(64)));
};

<<Auxiliary functions>>=
static inline u64 gemv(u64 x, const struct matmul_table_t * M)
{
	u64 r = 0;
	r ^= M->tables[0][x & 0x00ff];
	r ^= M->tables[1][(x >> 8) & 0x00ff];
	r ^= M->tables[2][(x >> 16) & 0x00ff];
	r ^= M->tables[3][(x >> 24) & 0x00ff];
	r ^= M->tables[4][(x >> 32) & 0x00ff];
	r ^= M->tables[5][(x >> 40) & 0x00ff];
	r ^= M->tables[6][(x >> 48) & 0x00ff];
	r ^= M->tables[7][(x >> 56) & 0x00ff];
	return r;
}

<<Release scratch space>>=
free(OUT);
free(COUNT);

<<Display preliminary info>>=
double expansion = (100.0 * (scratch_size - n)) / n;
printf("|scratch| = %d items (expansion = %.1f %%), tisze=%d, psize=%d\n", 
	scratch_size, expansion, tsize, psize);
double st1_part = (9.5367431640625e-07 * n) / fan_out;
printf("Expected stage-1 partition = %.1f Mb\n", st1_part);


<<Check partitioning output>>=
for (u32 i = 0; i < fan_out; i++)
	for (u32 t = 0; t < T; t++) {
		u32 start = psize * i + tsize * t;
		u64 *L = OUT + start;
		u32 n = COUNT[t * fan_out + i] - start;
		for (u32 j = 0; j < n; j++)
			assert((L[j] >> shift) == i);
	}

<<Run everything>>=
u32 T = omp_get_max_threads();
printf("# Benchmarking WC, p=%d, %d iterations, T=%d\n", p, iterations, T);
for (u32 nkilo = 1; nkilo < maxn+1; nkilo += 8) {
	u32 n = nkilo * 1024;
	<<Prepare scratch space>>
	#if product
	<<Prepare random matrix>>
	#endif
	<<Run actual code and measurements>>
	<<Release scratch space>>
}



<<Run actual code and measurements>>=
printf("%d; ", nkilo);
for (u32 it = 0; it < iterations; it++) {
	double start = wtime();	
	#pragma omp parallel
	{
		assert(T == (u32) omp_get_num_threads());
		u32 t = omp_get_thread_num();
		<<Partitioning with write-combining buffer>>
	}
	double duration = wtime() - start;
	double rate = 9.313225746154785e-10 * n / duration;
	printf("%.2f; ", rate);         /* displays N / duration in G item.bit / s */
	fflush(stdout);
	if (check) {
		<<Check partitioning output>>
	}
	if (update) {
		<<Modify input>>
	}
}
printf("\n");
// double duration = (wtime() - start) / (iterations - 1);
// u64 cycles = (ticks() - clock) / (iterations - 1);
// double rate = n / duration * 9.313225746154785e-10;
// double cpi = (1.0 * cycles) / n;
// double bw = 16 * n / duration * 9.313225746154785e-10;
// printf("\n");
// printf("p=%d, T=%d, %s: %.1fs | %.1fMitem/s | %.1f cycles/item | memory BW = %.2fGb/s | %.1f Mitem/s/bit\n", 
//					p, T, algo[a], duration, rate, cpi, bw, p * rate);


<<Modify input>>=
#pragma omp parallel for
for (u32 i = 1; i < n; i++)
	IN[i] += (0x1337 * IN[i - 1]) ^ (IN[i] >> 13);


@ \section{Actual Partitioning Code}

The simplest possible partitioning code is the following.
Note that a preliminary examination suggests that this loop should be unrolled.

<<Auxiliary functions>>=
static inline void direct_step(u64 x, const struct matmul_table_t *M, u32 shift, u32 *count, u64 *OUT)
{
	u64 y = product ? gemv(x, M) : x;
	u32 h = y >> shift;
	u32 idx = count[h]++;
	OUT[idx] = y;
}

<<Direct partitioning>>=
u32 *count = COUNT + t * fan_out;
for (u32 i = 0; i < fan_out; i++)
	count[i] = psize * i + tsize * t;
#pragma omp for schedule(static)
for (u32 i = 0; i < n; i++)
	direct_step(IN[i], &M, shift, count, OUT);


@ This direct approach has two problems:
\begin{enumerate}
\item There is a severe performance degradation when the fan-out is greater than 64 (which corresponds to the size
of the level-1 TLB). 
\item Performance scales \emph{very badly} when several cores are available.
\end{enumerate}

We benchmarked this code on a single 18-cores Xeon E5-2695 v4 @ 2.1Ghz CPU (we
used \texttt{numactl} to confine the code inside one NUMA node). The
``partitioning efficiency'' is the product of $\ell$ (the number of radix
bits) by the throughput of the procedure. The experiment was done on random
input lists of size $2^{27}$, which are relevant to us.

% \begin{figure}
% \begin{center}
% \includegraphics[width=0.75\textwidth]{partitioning_efficiency}
% \end{center}
% \caption{\label{fig:p_efficiency} Partitioning efficiency.}
% \end{figure}

% \begin{figure}
% \begin{center}
% \includegraphics[width=0.75\textwidth]{partitioning_scalability}
% \end{center}
% \caption{\label{fig:p_scalability} Scalability of partitioning.}
% \end{figure}

As can be seen on fig~\ref{fig:p_efficiency}, the fan-out has a large influence
on the speed of dispatching. If the fan-out exceeds the number of cache lines,
then writing to the output is likely to cause a cache miss. In addition, if the
fan-out if higher than the number of TLB entries, then each write may cause a
page walk. On most currently available CPUs, the L1 cache has 512 cache lines
(shared between two hardware contexts), the level-1 TLB has 64 entries and the
level-2 TLB has 512 entries. The direct partitioning efficiency drops when the
fan-out exceeds 64.

The TLB effect and scalability issue can be partially alleviated by a neat
implementation trick: the \emph{software write-combining buffer}. Each thread
allocates a buffer containing one cache line per bucket. Dispatched entries
are written to this buffer, instead of the [[OUT]] array. Because the buffer
is compact, it may fit in cache, and accessing it will not cause page walks.

When an entry of the buffer is full, it is flushed to the [[OUT]] array. This
may cause a TLB page walk, but several elements are then transfered at once,
so the penalty is amortized. This induce extra work, but the overall effect is
beneficial on large CPUs with many cores (we get the highest partitioning
efficiency with $\ell=10$, compared to $\ell=5$ with the direct approach). On
smaller CPUs, such as a laptop's Core i7-6600U, the effect of the write-
combining buffer \emph{cannot be observed}, and direct partitioning is always
much faster.

We use the write-combining buffer through these functions:

<<Function prototypes>>=
struct cacheline_t * wc_alloc(u32 fan_out);
void wc_prime(struct cacheline_t * buffer, u32 fan_out);
static inline void wc_push(struct cacheline_t * buffer, u32 fan_out, u64 *OUT, u64 x, u32 h);
void wc_flush(struct cacheline_t * buffer, u32 fan_out, u64 *OUT);
void wc_get_count(struct cacheline_t * buffer, u32 fan_out, u32 *COUNT);

@ Here is the full partitioning code:

<<Auxiliary functions>>=
static inline void wc_step(u64 x, const struct matmul_table_t *M, u32 shift, struct cacheline_t *buffer, u32 fan_out, u64 *OUT)
{
	u64 y = product ? gemv(x, M) : x;
	u32 h = y >> shift;
	wc_push(buffer, fan_out, OUT, y, h);
}

<<Partitioning with write-combining buffer>>=
assert(sizeof(struct cacheline_t) == CACHE_LINE_SIZE);
struct cacheline_t * buffer = wc_alloc(fan_out);
for (u32 i = 0; i < fan_out; i++)
	buffer[i].start = psize * i + t * tsize;
wc_prime(buffer, fan_out);
#pragma omp for schedule(static)
for (u32 i = 0; i < n; i++)
	wc_step(IN[i], &M, shift, buffer, fan_out, OUT);
wc_flush(buffer, fan_out, OUT);
wc_get_count(buffer, fan_out, COUNT + t * fan_out);
free(buffer);

@ We now look at the inner workings of the buffer. It is an array of $2^p$
[[cacheline_t]] objects, whose size should match that of a cache line on the
CPU.

<<Type definitions>>=
struct cacheline_t {
        u64 values[7];
        u32 start;
        u32 index;
};

@ The [[cacheline_t]] objects temporarily hold items that are destined to land
at a specified position in the [[OUT]] array. The [[start]] field is the
offset in [[OUT]] where the first incoming item entering the [[cacheline_t]]
should be written, while [[index]] contains the index in the [[value]] array
of this [[cacheline_t]] where the next item should be written. 

We enforce that if a value enters a [[cacheline_t]] are is supposed to land in
[[OUT[x]]], then it is stored in [[values[x & 7]]]. It follows that a
[[cacheline_t]] contains meaningful entries in [[values[(start & 7):index]]].
When a [[cacheline_t]] gets full, \textit{i.e.} when [[index]] would reach 8,
it is flushed to the [[OUT]] array.

When the dispatching procedure starts, all the [[cacheline_t]] are primed:
their [[start]] field  is pointed to the corresponding thread-bucket inside
[[OUT]] (no pun intended), using the values in [[COUNT]].

To use the write-combining buffer, one has to:
\begin{enumerate}
\item Allocate space for the buffer itself, at a cache-line boundary.
\item Set the offset of the $i$-th bucket in [[buffer[i].start]].
\item Derive [[index]] from [[start]] (the ``Finalization'').
\end{enumerate}

<<Auxiliary functions>>=
struct cacheline_t * wc_alloc(u32 fan_out)
{
	struct cacheline_t *buffer = aligned_alloc(CACHE_LINE_SIZE, sizeof(*buffer) * fan_out);
	if (buffer == NULL)
		err(1, "aligned allocation of write-combining buffer");
	return buffer;
}

<<Auxiliary functions>>=
void wc_prime(struct cacheline_t * buffer, u32 fan_out)
{
	for (u32 i = 0; i < fan_out; i++ )
		buffer[i].index = buffer[i].start & 7;
}

@ All items enter the buffer through the [[wc_push]] function, with [[h]]
indicating in which bucket they should land. Writing in [[value[7]]] would
overwrite [[start]] and [[index]], so we take care to preserve them.

<<Auxiliary functions>>=
static inline void wc_push(struct cacheline_t * buffer, u32 fan_out, u64 *OUT, u64 x, u32 h)
{
	(void) fan_out;
	u32 index = buffer[h].index;
	u32 start = buffer[h].start;
	buffer[h].values[index] = x;
	if (index == 7) {
		<<Flush [[buffer[h]]] to [[OUT]] and re-prime it>>
	} else {
		buffer[h].index = index + 1;
	}
}

@ Flushing is the tricky part. If [[start]] is aligned on a cache-line
boundary, then the whole cache-line is flushed using a \emph{non-temporal
store} (that does not leave the data in L2 cache, since we won't use it
anytime soon). If the cacheline is only partially full, then its elements are
flushed one-by-one. This \emph{requires} however that [[OUT]] is aligned on a
cache-line boundary.

<<Function prototypes>>=
void store_nontemp_64B(void * dst, void * src);

<<Auxiliary functions>>=
void store_nontemp_64B(void * dst, void * src)
{
    register __m256i * d1 = (__m256i*) dst;
    register __m256i s1 = *((__m256i*) src);
    register __m256i * d2 = d1+1;
    register __m256i s2 = *(((__m256i*) src)+1);
    _mm256_stream_si256(d1, s1);
    _mm256_stream_si256(d2, s2);
}


<<Flush [[buffer[h]]] to [[OUT]] and re-prime it>>=
if ((start & 7) == 0) {
	store_nontemp_64B(&OUT[start], &buffer[h]);
	start += 8;
} else {
	while ((start & 7) != 0) {
		OUT[start] = buffer[h].values[start & 7];
		start += 1;
	}
}
buffer[h].start = start;
buffer[h].index = 0;
	
@  TODO : check if a [[u64 meta]] is faster.
 
@ Once all the input has been processed, some values remain in the buffer, and
they also need to be flushed to [[OUT]] one-by-one.

<<Auxiliary functions>>=
void wc_flush(struct cacheline_t * buffer, u32 fan_out, u64 *OUT)
{
	for (u32 i = 0; i < fan_out; i++) {
		u32 start = buffer[i].start;
		u32 index = buffer[i].index;
		while ((start & 7) < index) {
			OUT[start] = buffer[i].values[start & 7];
			start += 1;
		}
		buffer[i].start = start;
	}
}

@ Once the buffer has been flushed, it may safely be deallocated. However, we 
need to read the size of individual buckets from it.

<<Auxiliary functions>>=
void wc_get_count(struct cacheline_t * buffer, u32 fan_out, u32 *COUNT)
{
	for (u32 i = 0; i < fan_out; i++)
		COUNT[i] = buffer[i].start;
}

@ \section{Results}

\subsection{8-threads Intel Xeon E5-4620 @ 2.20GHz (hpac)}

\includegraphics[width=\textwidth]{partition_p_hpac}
\includegraphics[width=\textwidth]{partition_p_hpac_gemv}
\includegraphics[width=\textwidth]{partition_T_hpac}
\includegraphics[width=\textwidth]{partition_T_hpac_gemv}

\subsection{18-threads (+HT) Intel Xeon E5-2695 v4 @ 2.10GHz (ppti-gpu-3)}

\includegraphics[width=\textwidth]{partition_p_ppti_gpu_3}
\includegraphics[width=\textwidth]{partition_p_ppti_gemv}
\includegraphics[width=\textwidth]{partition_T_ppti_gpu_3}
\includegraphics[width=\textwidth]{partition_T_ppti_gemv}

\subsection{4-threads AMD EPYC 7301 @ 2.20GHz (+HT)  (G5K chiclet)}

\includegraphics[width=\textwidth]{partition_p_zen2}
\includegraphics[width=\textwidth]{partition_p_zen2_gemv}
\includegraphics[width=\textwidth]{partition_T_zen2}
\includegraphics[width=\textwidth]{partition_T_zen2_gemv}

\section{Conclusion}

Using the STREAM benchmark we estimated the ``all threads'' copy bandwidth 
of the three machines (on numa node 0): 

\begin{tabular}{|c||c|c|c|c|}
\hline
machine                 & [[hpac]] & [[ppti-gpu-3]] & [[chiclet]] & [[turing]] \\
\hline
\hline
cores (numa node 0)     & 8        & 18             & 4           & 16 \\
SMT ?                   & no       & 2              & 2           & 4  \\
memory BW (GB/s)        & 22       & 26             & 15          & 42 \\
BW / thread             & 2.75     & 0.7            & 3.75        & 0.6 \\ 
\hline
max rate (Gitem/s)      & 1.37     & 1.6            & 0.9         & 2.6 \\
\hline
observed rate (Gitem/s) & 1.15     & 1.65           & 0.85        & ?\\
\hline
observed GEMV (Gitem/s) & 0.575    & 1.65           & 0.525       & ?\\
\hline
bound by                & compute  & memory         & compute     & ?\\
\hline	
\end{tabular}



\begin{itemize}
\item The write-combining buffer is always a win.
\item Using all available hardware threads is the way to go.
\item Performing GEMV halves the processing speed... unless memory BW is the limit.
\item Unless so many threads are available that the memory bandwidth can be saturated.
\item Scalability is good, up to a limit.
\item Using the WC, there is a largish band for $1 \leq p \leq 10$ where GEMV+partitioning is fast
(the actual upper-bound depends on the machine).
\end{itemize}

@ \end{document}