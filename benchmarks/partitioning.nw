\documentclass{book}

\usepackage{geometry}
\usepackage{noweb}
\usepackage{amsmath,amsfonts,amssymb}
\usepackage{parskip}
\usepackage{xspace}
\usepackage{hyperref}
\usepackage{tikz}
\usetikzlibrary{patterns}

\newcommand{\join}{\bowtie}
\newcommand{\OMP}{\textsf{OpenMP}\xspace}

\begin{document}

One iteration of Joux's algorithm can essentially broken down in two stages. 
\begin{enumerate}
\item Matrix multiplication and stage-1 partitioning.
\item Stage-2 partitioning and hash joins.
\end{enumerate}

These two stages are quite different.

Here are the knobs we can turn:
\begin{itemize}
\item Size of the input lists (it seems that it doesn't matter, except that variability increases for short lists)
\item Number of slices for GEMV
\item stage-1 partitioning bits
\item stage-2 partitioning bits (potentially zero!)
\item How to perform the sub-joins
\end{itemize}

%\setcounter{chapter}{2}
\chapter{Partitioning Benchmark}

Let's start will all the useless stuff.

<<*>>=
#define _XOPEN_SOURCE
#include <math.h>
#include <stdlib.h>
#include <stdio.h>
#include <assert.h>
#include <err.h>
#include <sys/types.h>
#include <sys/stat.h>
#include <getopt.h>

#include <omp.h>
#include <mpi.h>
// #include <papi.h>

#if __AVX__
#include <immintrin.h>
#endif

#include "common.h"


<<Settings>>
<<Type definitions>>
<<Function prototypes>>
<<Auxiliary functions>>
<<The main function>>

<<Settings>>=
#define check false
#define update false
#define product false
#define separate_product true


<<The main function>>=
int main(int argc, char **argv)
{
	<<Setup MPI>>
	if (rank == 0) {
		if (check)
			warnx("CHECK IS ON");
		if (update)
			warnx("UPDATE IS ON");
		if (product)
			warnx("PRODUCT IS ON");
	}
	<<Process the command line>>
	<<Prepare input data>>
	<<Run everything>>
	MPI_Finalize();
}


<<Setup MPI>>=
int rank;
MPI_Init(&argc, &argv);
MPI_Comm_rank(MPI_COMM_WORLD, &rank);

<<Process the command line>>=
struct option longopts[4] = {
	{"p", required_argument, NULL, 'k'},
	{"n", required_argument, NULL, 'n'},
	{"it", required_argument, NULL, 'i'},
	{NULL, 0, NULL, 0}
};
u32 maxp = 15;
u64 n = 128 * 1024 * 1024;  /* in octabytes */
u32 iterations = 10;
signed char ch;
while ((ch = getopt_long(argc, argv, "", longopts, NULL)) != -1) {
    	switch (ch) {
	case 'k':
		maxp = atoi(optarg);
		break;
	case 'n':
		n = atoll(optarg);
		break;
	case 'i':
		iterations = atoi(optarg);
		break;
	default:
		errx(1, "Unknown option\n");
	}
}
if (maxp == 0)
	errx(1, "missing option --p");


<<Auxiliary functions>>=
static const u32 CACHE_LINE_SIZE = 64;

u64 ROUND(u64 s)
{
	return CACHE_LINE_SIZE * ceil(((double) s) / CACHE_LINE_SIZE);
}

u64 chernoff_bound(u64 N, u32 n_buckets)
{
	double mu = ((double) N) / n_buckets;
	double delta = sqrt(210 / mu);
	return ROUND(N * (1 + delta) / n_buckets);
}



<<Prepare input data>>=
if (rank == 0)
	printf("# Loading random junk\n");
u32 size = sizeof(u64) * ROUND(n);
u64 *IN = aligned_alloc(64, size);
if (IN == NULL)
	err(1, "cannot allocate memory for IN");
u8 state[256];
RC4_keysetup(state, 0);
RC4_stream(state, (u8 *) IN, size);


<<Prepare scratch space>>=
u8 shift = 64 - p;
u32 fan_out = 1 << p;
u32 tsize = chernoff_bound(n, T * fan_out);
u32 psize = tsize * T;
u32 scratch_size = psize * fan_out;
u64 *OUT = aligned_alloc(64, sizeof(*OUT) * scratch_size);
u32 *COUNT = aligned_alloc(64, ROUND(sizeof(*COUNT) * fan_out * T));
if (OUT == NULL || COUNT == NULL)
	err(1, "cannot allocate memory for OUT or COUNT");


<<Prepare random matrix>>=
struct matmul_table_t M;
for (u32 i = 0; i < 8; i++)
	for (u32 j = 0; j < 256; j++)
	M.tables[i][j] = mrand48() ^ (((u64) mrand48()) << 32);


<<Type definitions>>=
struct matmul_table_t {
	u64 tables[8][256] __attribute__((aligned(64)));
};

<<Auxiliary functions>>=
static inline u64 gemv(u64 x, const struct matmul_table_t * M)
{
	u8 a = x;
	u8 b = x >> 8;
	u8 c = x >> 16;
	u8 d = x >> 24;
	u8 e = x >> 32;
	u8 f = x >> 40;
	u8 g = x >> 48;
	u8 h = x >> 56;
	
	u64 r = M->tables[0][a];
	r ^= M->tables[1][b];
	r ^= M->tables[2][c];
	r ^= M->tables[3][d];
	r ^= M->tables[4][e];
	r ^= M->tables[5][f];
	r ^= M->tables[6][g];
	r ^= M->tables[7][h];
	return r;
}

<<Release scratch space>>=
free(OUT);
free(COUNT);

<<Display preliminary info>>=
double expansion = (100.0 * (scratch_size - size)) / size;
printf("# |scratch| = %d items (expansion = %.1f %%), tisze=%d, psize=%d\n", 
	scratch_size, expansion, tsize, psize);
//double st1_part = (9.5367431640625e-07 * n) / fan_out;
//printf("# Expected stage-1 partition = %.1f Mb\n", st1_part);


<<Check partitioning output>>=
for (u32 i = 0; i < fan_out; i++)
	for (u32 t = 0; t < T; t++) {
		u32 start = psize * i + tsize * t;
		u64 *L = OUT + start;
		u32 n = COUNT[t * fan_out + i] - start;
		for (u32 j = 0; j < n; j++)
			assert((L[j] >> shift) == i);
	}

<<Run everything>>=
const char *algo[2] = {"direct", "write-combining"};
u32 maxT = omp_get_max_threads();
for (u32 a = 0; a < 2; a++) {
	if (rank == 0)
		printf("# Benchmarking %s, %d iterations\n", algo[a], iterations);
	for (u32 T = 4; T <= maxT; T += 4) {
		if (rank == 0)
			printf("# using T = %d threads\n", T);
		omp_set_num_threads(T);
		for (u32 p = 1; p < maxp; p++) {
			<<Prepare scratch space>>
			if (rank == 0) {
				<<Display preliminary info>>
			}
			<<Prepare random matrix>>
			<<Run actual code and measurements>>
			<<Release scratch space>>
		}
	}
}



<<Run actual code and measurements>>=
if (rank == 0)
	printf("%s; %d; %d; ", algo[a], p, T);
for (u32 it = 0; it < iterations; it++) {
	double start = wtime();	
	#pragma omp parallel
	{
		if (separate_product) {
			#pragma omp for schedule(static)
			for (u64 i = 0; i < n; i++)
                		IN[i] = gemv(IN[i], &M);
		}
		assert(T == (u32) omp_get_num_threads());
		u32 t = omp_get_thread_num();
		if (a == 0) {
			<<Direct partitioning>>
		} else {
			<<Partitioning with write-combining buffer>>
		}
	}
	if (rank == 0) {
		double duration = wtime() - start;
		double rate = 9.5367431640625e-07 * n / duration;
		printf("%.2f; ", rate);         /* displays N / duration in M item / s */
		fflush(stdout);
	}
	if (check) {
		<<Check partitioning output>>
	}
	if (update) {
		<<Modify input>>
	}
}
if (rank == 0)
	printf("\n");


<<Modify input>>=
#pragma omp parallel for
for (u32 i = 1; i < n; i++)
	IN[i] += (0x1337 * IN[i - 1]) ^ (IN[i] >> 13);


@ \section{Actual Partitioning Code}

The simplest possible partitioning code is the following.
Note that a preliminary examination suggests that this loop should be unrolled.

<<Auxiliary functions>>=
static inline void direct_step(u64 x, const struct matmul_table_t *M, u32 shift, u32 *count, u64 *OUT)
{
	u64 y = product ? gemv(x, M) : x;
	u32 h = y >> shift;
	u32 idx = count[h]++;
	OUT[idx] = y;
}

<<Direct partitioning>>=
u32 *count = COUNT + t * fan_out;
for (u32 i = 0; i < fan_out; i++)
	count[i] = psize * i + tsize * t;
#pragma omp for schedule(static)
for (u32 i = 0; i < n; i++)
	direct_step(IN[i], &M, shift, count, OUT);


@ This direct approach has two problems:
\begin{enumerate}
\item There is a severe performance degradation when the fan-out is greater than 64 (which corresponds to the size
of the level-1 TLB). 
\item Performance scales \emph{very badly} when several cores are available.
\end{enumerate}

The fan-out has a large influence
on the speed of dispatching. If the fan-out exceeds the number of cache lines,
then writing to the output is likely to cause a cache miss. In addition, if the
fan-out if higher than the number of TLB entries, then each write may cause a
page walk. On most currently available CPUs, the L1 cache has 512 cache lines
(shared between two hardware contexts), the level-1 TLB has 64 entries and the
level-2 TLB has 512 entries. The direct partitioning efficiency drops when the
fan-out exceeds 64.

The TLB effect and scalability issue can be partially alleviated by a neat
implementation trick: the \emph{software write-combining buffer}. Each thread
allocates a buffer containing one cache line per bucket. Dispatched entries
are written to this buffer, instead of the [[OUT]] array. Because the buffer
is compact, it may fit in cache, and accessing it will not cause page walks.

When an entry of the buffer is full, it is flushed to the [[OUT]] array. This
may cause a TLB page walk, but several elements are then transfered at once,
so the penalty is amortized. This induce extra work, but the overall effect is
beneficial on large CPUs with many cores (we get the highest partitioning
efficiency with $\ell=10$, compared to $\ell=5$ with the direct approach). On
smaller CPUs, such as a laptop's Core i7-6600U, the effect of the write-
combining buffer \emph{cannot be observed}, and direct partitioning is always
much faster.

We use the write-combining buffer through these functions:

<<Function prototypes>>=
struct cacheline_t * wc_alloc(u32 fan_out);
void wc_prime(struct cacheline_t * buffer, u32 fan_out);
static inline void wc_push(struct cacheline_t * buffer, u64 *OUT, u64 x, u32 h);
void wc_flush(struct cacheline_t * buffer, u32 fan_out, u64 *OUT);
void wc_get_count(struct cacheline_t * buffer, u32 fan_out, u32 *COUNT);

@ Here is the full partitioning code:

<<Auxiliary functions>>=
static inline void wc_step(u64 x, const struct matmul_table_t *M, u32 shift, struct cacheline_t *buffer, u64 *OUT)
{
	u64 y = product ? gemv(x, M) : x;
	u32 h = y >> shift;
	wc_push(buffer, OUT, y, h);
}

<<Partitioning with write-combining buffer>>=
assert(sizeof(struct cacheline_t) == CACHE_LINE_SIZE);
struct cacheline_t * buffer = wc_alloc(fan_out);
for (u32 i = 0; i < fan_out; i++)
	buffer[i].values[7] = psize * i + t * tsize;
wc_prime(buffer, fan_out);

#pragma omp for schedule(static)
for (u32 i = 0; i < n; i++)
	wc_step(IN[i], &M, shift, buffer, OUT);

wc_flush(buffer, fan_out, OUT);
wc_get_count(buffer, fan_out, COUNT + t * fan_out);
free(buffer);

@ We now look at the inner workings of the buffer. It is an array of $2^p$
[[cacheline_t]] objects, whose size should match that of a cache line on the
CPU.

<<Type definitions>>=
struct cacheline_t {
	u64 values[8];
};
/*union cacheline_t {
        u64 values[8];
        struct {
        	u64 values[7];
        	u32 start;        // the distinction between these two is useless
        	u32 index;        // when the output buckets are cacheline-aligned
        } data;
};*/


@ The [[cacheline_t]] objects temporarily hold items that are destined to land
at a specified position in the [[OUT]] array. The [[start]] field is the
offset in [[OUT]] where the first incoming item entering the [[cacheline_t]]
should be written, while [[index]] contains the index in the [[value]] array
of this [[cacheline_t]] where the next item should be written. 

We enforce that if a value enters a [[cacheline_t]] are is supposed to land in
[[OUT[x]]], then it is stored in [[values[x & 7]]]. It follows that a
[[cacheline_t]] contains meaningful entries in [[values[(start & 7):index]]].
When a [[cacheline_t]] gets full, \textit{i.e.} when [[index]] would reach 8,
it is flushed to the [[OUT]] array.

When the dispatching procedure starts, all the [[cacheline_t]] are primed:
their [[start]] field  is pointed to the corresponding thread-bucket inside
[[OUT]] (no pun intended), using the values in [[COUNT]].

To use the write-combining buffer, one has to:
\begin{enumerate}
\item Allocate space for the buffer itself, at a cache-line boundary.
\item Set the offset of the $i$-th bucket in [[buffer[i].start]].
\item Derive [[index]] from [[start]] (the ``Finalization'').
\end{enumerate}

<<Auxiliary functions>>=
struct cacheline_t * wc_alloc(u32 fan_out)
{
	assert(sizeof(struct cacheline_t) == CACHE_LINE_SIZE);
	struct cacheline_t *buffer = aligned_alloc(CACHE_LINE_SIZE, sizeof(*buffer) * fan_out);
	if (buffer == NULL)
		err(1, "aligned allocation of write-combining buffer");
	return buffer;
}

<<Auxiliary functions>>=
void wc_prime(struct cacheline_t * buffer, u32 fan_out)
{
	for (u32 i = 0; i < fan_out; i++ )
		assert((buffer[i].values[7] & 7) == 0);
		// buffer[i].index = buffer[i].start & 7;
}

@ All items enter the buffer through the [[wc_push]] function, with [[h]]
indicating in which bucket they should land. Writing in [[value[7]]] would
overwrite [[start]] and [[index]], so we take care to preserve them.

<<Auxiliary functions>>=
static inline void wc_push(struct cacheline_t * buffer, u64 *OUT, u64 x, u32 h)
{
	u64 meta = buffer[h].values[7];
	u64 index = meta & 7;
	buffer[h].values[index] = x;
	if (index == 7) {
		u64 start = meta & 0xfffffffffffffff8;
		store_nontemp_64B(&OUT[start], buffer[h].values);
	}
	buffer[h].values[7] = meta + 1;
}

@ Flushing is the tricky part. If [[start]] is aligned on a cache-line
boundary, then the whole cache-line is flushed using a \emph{non-temporal
store} (that does not leave the data in L2 cache, since we will not use it
anytime soon). If the cacheline is only partially full, then its elements are
flushed one-by-one. This \emph{requires} however that [[OUT]] is aligned on a
cache-line boundary.

We only do this for CPUs with AVX.

<<Function prototypes>>=
void store_nontemp_64B(void * dst, void * src);

<<Auxiliary functions>>=
void store_nontemp_64B(void * dst, void * src)
{
#if __AVX__
	register __m256i * d1 = (__m256i*) dst;
	register __m256i s1 = *((__m256i*) src);
	register __m256i * d2 = d1+1;
	register __m256i s2 = *(((__m256i*) src)+1);
	_mm256_stream_si256(d1, s1);
	_mm256_stream_si256(d2, s2);
	// note : on peut aussi le faire en SSE
#else
	u64 *in = src;
	u64 *out = dst;
	u64 a = in[0];
	u64 b = in[1];
	u64 c = in[2];
	u64 d = in[3];
	u64 e = in[4];
	u64 f = in[5];
	u64 g = in[6];
	u64 h = in[7];
	out[0] = a; 
	out[1] = b;
	out[2] = c;
	out[3] = d;
	out[4] = e;
	out[5] = f;
	out[6] = g;
	out[7] = h;
#endif
}

@ Once all the input has been processed, some values remain in the buffer, and
they also need to be flushed to [[OUT]] one-by-one.

<<Auxiliary functions>>=
void wc_flush(struct cacheline_t * buffer, u32 fan_out, u64 *OUT)
{
	for (u32 i = 0; i < fan_out; i++) {
		u64 meta = buffer[i].values[7];
		u64 start = meta & 0xfffffffffffffff8;
		for (u64 j = start; j < meta; j++)
			OUT[j] = buffer[i].values[j & 7];
	}
}

@ Once the buffer has been flushed, it may safely be deallocated. However, we 
need to read the size of individual buckets from it.

<<Auxiliary functions>>=
void wc_get_count(struct cacheline_t * buffer, u32 fan_out, u32 *COUNT)
{
	for (u32 i = 0; i < fan_out; i++)
		COUNT[i] = buffer[i].values[7];
}

@ \section{Instrumentation}

<<Start PAPI counters>>=
int events[4] = {
	PAPI_L1_LDM, // 0x80000017 Yes No Level 1 load misses
	// PAPI_TLB_DM, // 0x80000014 Yes Yes Data translation lookaside buffer misses
	PAPI_LD_INS, //  0x80000035  Yes   No   Load instructions
	PAPI_TOT_INS, // 0x80000032 Yes No Instructions completed
	PAPI_TOT_CYC, // 0x8000003b Yes No Total cycles
};
int rc = PAPI_start_counters(events, 4);
if (rc < PAPI_OK)
	errx(1, "PAPI_start_counters : %d", rc);


<<Stop PAPI counters and report>>=
rc = PAPI_stop_counters(counters, 4);
if (rc < PAPI_OK)
    	errx(1, "PAPI_stop_counters : %d", rc);
printf("[tid=%d] PAPI_L1_LDM = %lld\n", t, counters[0]);
// printf("[tid=%d] PAPI_TLB_DM = %lld\n", t, counters[1]);
printf("[tid=%d] PAPI_LD_INS = %lld\n", t,  counters[1]);
printf("[tid=%d] PAPI_TOT_INS = %lld\n", t,  counters[2]);
printf("[tid=%d] PAPI_TOT_CYC = %lld\n", t,  counters[3]);

@ \section{Results}

\subsection{8-threads Intel Xeon E5-4620 @ 2.20GHz (hpac)}

\includegraphics[width=\textwidth]{partition_p_hpac}
\includegraphics[width=\textwidth]{partition_p_hpac_gemv}
\includegraphics[width=\textwidth]{partition_T_hpac}
\includegraphics[width=\textwidth]{partition_T_hpac_gemv}

\subsection{18-threads (+HT) Intel Xeon E5-2695 v4 @ 2.10GHz (ppti-gpu-3)}

\includegraphics[width=\textwidth]{partition_p_ppti_gpu_3}
\includegraphics[width=\textwidth]{partition_p_ppti_gemv}
\includegraphics[width=\textwidth]{partition_T_ppti_gpu_3}
\includegraphics[width=\textwidth]{partition_T_ppti_gemv}

\subsection{4-threads AMD EPYC 7301 @ 2.20GHz (+HT)  (G5K chiclet)}

\includegraphics[width=\textwidth]{partition_p_zen2}
\includegraphics[width=\textwidth]{partition_p_zen2_gemv}
\includegraphics[width=\textwidth]{partition_T_zen2}
\includegraphics[width=\textwidth]{partition_T_zen2_gemv}

\section{Conclusion}

Using the STREAM benchmark we estimated the ``all threads'' copy bandwidth 
of the three machines (on numa node 0): 

\begin{tabular}{|c||c|c|c|c|}
\hline
machine                 & [[hpac]] & [[ppti-gpu-3]] & [[chiclet]] & [[turing]] \\
\hline
\hline
cores (numa node 0)     & 8        & 18             & 4           & 16 \\
SMT ?                   & no       & 2              & 2           & 4  \\
memory BW (GB/s)        & 22       & 26             & 15          & 26 \\
BW / thread             & 2.75     & 0.7            & 3.75        & 0.6 \\ 
\hline
max rate (Gitem/s)      & 1.37     & 1.6            & 0.9         & 1.6 \\
\hline
observed rate (Gitem/s) & 1.15     & 1.65           & 0.85        & 1.6 \\
\hline
observed w/ GEMV (Gitem/s) & 0.575 & 1.65           & 0.525       & 0.5\\
\hline
bound by                & compute  & memory         & compute     & compute \\
\hline	
\end{tabular}

\begin{itemize}
\item The write-combining buffer is always a win \emph{EXCEPT ON THE BlueGene}.
\item Using all available hardware threads is the way to go (EXCEPT : BG/Q. Partitioning is best at 3 threads/core).
\item Performing GEMV halves the processing speed... unless memory BW is the limit.
\item Unless so many threads are available that the memory bandwidth can be saturated.
\item Scalability is good, up to a limit.
\item Using the WC, there is a largish band for $1 \leq p \leq 10$ where GEMV+partitioning is fast
(the actual upper-bound depends on the machine).
\item On the BG/Q, doing GEMV first THEN partitioning is a bit faster. To investigate (compute bound in parallel with memory-bound ?)
\end{itemize}

@ \end{document}